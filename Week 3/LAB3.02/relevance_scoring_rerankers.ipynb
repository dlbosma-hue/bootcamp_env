{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254d3adc",
   "metadata": {},
   "source": [
    "### LAB | Relevance Scoring and Rerankers for Trustworthy AI & Repository Ai Literacy\n",
    "Dina Bosma-Buczynska"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671e70b",
   "metadata": {},
   "source": [
    "**Step 1: Setup and Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install openai pinecone cohere python-dotenv langchain langchain-openai langchain-community langchain-text-splitters tiktoken pypdf requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09b9945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client ready: True\n",
      "Cohere client ready: True\n",
      "Pinecone client ready: True\n"
     ]
    }
   ],
   "source": [
    "# Load API keys and set up clients\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import cohere\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up clients\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "cohere_client = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "print(\"OpenAI client ready:\", openai_client is not None)\n",
    "print(\"Cohere client ready:\", cohere_client is not None)\n",
    "print(\"Pinecone client ready:\", pc is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af658ee8",
   "metadata": {},
   "source": [
    "Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf80dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"Living_Repository_AI_Literacy_Practices_Update_16042025_UqmogIt2HpLVokdcuzJL4mDvHk8_112203.pdf\")\n",
    "pdf_documents = pdf_loader.load()\n",
    "\n",
    "print(len(pdf_documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d2552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(\"chunked_transcription.txt\")\n",
    "podcast_documents = text_loader.load()\n",
    "\n",
    "print(len(podcast_documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63359e7d",
   "metadata": {},
   "source": [
    "Add Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d34c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_documents:\n",
    "    doc.metadata[\"source\"] = \"eu_ai_act\"\n",
    "    doc.metadata[\"document_type\"] = \"legal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d08ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in podcast_documents:\n",
    "    doc.metadata[\"source\"] = \"podcast\"\n",
    "    doc.metadata[\"document_type\"] = \"transcript\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2a58b",
   "metadata": {},
   "source": [
    "*Chunk Documents:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f58274",
   "metadata": {},
   "source": [
    "Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52d1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1a7c4",
   "metadata": {},
   "source": [
    "Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8d04e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 320\n"
     ]
    }
   ],
   "source": [
    "pdf_chunks = text_splitter.split_documents(pdf_documents)\n",
    "podcast_chunks = text_splitter.split_documents(podcast_documents)\n",
    "\n",
    "all_chunks = pdf_chunks + podcast_chunks\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cdd17",
   "metadata": {},
   "source": [
    "Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103956d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9f3c2",
   "metadata": {},
   "source": [
    "**Step 2: Generate Embeddings and Initial Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3afb6a",
   "metadata": {},
   "source": [
    "Create the Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-pinecone-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinabosmabuczynska/Desktop/bootcamp_env/.conda/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ready\n",
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '186',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Thu, 19 Feb 2026 16:50:27 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '60',\n",
      "                                    'x-pinecone-request-latency-ms': '60',\n",
      "                                    'x-pinecone-response-duration-ms': '62'}},\n",
      " 'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 960}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 960,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"trustworthy-ai-rag\"\n",
    "\n",
    "# Create index if it doesn't exist yet\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    # Wait until index is ready before proceeding\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(\"Index ready\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb63961",
   "metadata": {},
   "source": [
    "Embed All Chunks and Upload to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embed-and-upsert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index cleared.\n",
      "Embedded 20/320 chunks...\n",
      "Embedded 40/320 chunks...\n",
      "Embedded 60/320 chunks...\n",
      "Embedded 80/320 chunks...\n",
      "Embedded 100/320 chunks...\n",
      "Embedded 120/320 chunks...\n",
      "Embedded 140/320 chunks...\n",
      "Embedded 160/320 chunks...\n",
      "Embedded 180/320 chunks...\n",
      "Embedded 200/320 chunks...\n",
      "Embedded 220/320 chunks...\n",
      "Embedded 240/320 chunks...\n",
      "Embedded 260/320 chunks...\n",
      "Embedded 280/320 chunks...\n",
      "Embedded 300/320 chunks...\n",
      "Embedded 320/320 chunks...\n",
      "Upserted batch 1/4\n",
      "Upserted batch 2/4\n",
      "Upserted batch 3/4\n",
      "Upserted batch 4/4\n",
      "\n",
      "Done! Index stats:\n",
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '186',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Thu, 19 Feb 2026 16:51:25 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '41',\n",
      "                                    'x-pinecone-request-latency-ms': '40',\n",
      "                                    'x-pinecone-response-duration-ms': '42'}},\n",
      " 'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 320}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 320,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Clear the index first so re-running this cell never creates duplicates\n",
    "index.delete(delete_all=True)\n",
    "print(\"Index cleared.\")\n",
    "\n",
    "# Build vectors with a stable content-based ID to prevent duplicates\n",
    "vectors = []\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    embedding = get_embedding(chunk.page_content)\n",
    "    chunk_id = hashlib.md5(chunk.page_content.encode()).hexdigest()\n",
    "    vectors.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {\n",
    "            \"text\": chunk.page_content,\n",
    "            \"source\": chunk.metadata.get(\"source\", \"\"),\n",
    "            \"document_type\": chunk.metadata.get(\"document_type\", \"\"),\n",
    "            \"page\": chunk.metadata.get(\"page\", 0)\n",
    "        }\n",
    "    })\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Embedded {i+1}/{len(all_chunks)} chunks...\")\n",
    "\n",
    "# Upsert to Pinecone in batches of 100\n",
    "batch_size = 100\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    index.upsert(vectors=vectors[i:i+batch_size])\n",
    "    print(f\"Upserted batch {i//batch_size + 1}/{-(-len(vectors)//batch_size)}\")\n",
    "\n",
    "print(\"\\nDone! Index stats:\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-md",
   "metadata": {},
   "source": [
    "Baseline retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baseline-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 results for: 'What are the requirements for trustworthy AI?'\n",
      "\n",
      "Result 1 | Score: 0.8561 | Source: eu_ai_act\n",
      "  for overseeing the ethical and responsible use of AI across all projects , ensuring compliance with \n",
      "regulatory requirements, and mitigating potential risks....\n",
      "\n",
      "Result 2 | Score: 0.8822 | Source: podcast\n",
      "  me. It says, trust is something we give to people. We trust people. We rely on machines. That's the crux of it, isn't it? If an AI is a black box that we can't explain, we can't truly trust it, we're ...\n",
      "\n",
      "Result 3 | Score: 0.8533 | Source: podcast\n",
      "  to make it concrete, the experts derive four ethical principles from those rights. These are the non-negotiables. Okay, let's run through them. Principle one. Respect for human autonomy. This means hu...\n",
      "\n",
      "Result 4 | Score: 0.8605 | Source: eu_ai_act\n",
      "  use of the system(s). \n",
      "The SAS course on “ Responsible Innovation and Trustworthy AI ” is designed for anyone who wants to \n",
      "gain a deeper understanding about the importance of trust and responsibility...\n",
      "\n",
      "Result 5 | Score: 0.8640 | Source: podcast\n",
      "  phase. Today we are digging into the blueprint for that trust. We're unpacking a document that is arguably the Magna Carta for ethical computing, the ethics guidelines for trustworthy AI. This is a he...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, top_k=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return results[\"matches\"]\n",
    "\n",
    "# Test baseline retrieval\n",
    "query = \"What are the requirements for trustworthy AI?\"\n",
    "baseline_results = retrieve(query, top_k=10)\n",
    "\n",
    "print(f\"Retrieved {len(baseline_results)} results for: '{query}'\\n\")\n",
    "for i, match in enumerate(baseline_results[:5]):\n",
    "    print(f\"Result {i+1} | Score: {match['score']:.4f} | Source: {match['metadata']['source']}\")\n",
    "    print(f\"  {match['metadata']['text'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-llm-md",
   "metadata": {},
   "source": [
    "**Step 3: Implement Relevance Scoring via an LLM**\n",
    "\n",
    "Use GPT to score each retrieved chunk's relevance to the query, combine with the similarity score, then reorder results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa34b65",
   "metadata": {},
   "source": [
    "Score Each Chunk With GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "step3-llm-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring chunks with LLM... (this may take a moment)\n",
      "\n",
      "============================================================\n",
      "LLM RELEVANCE SCORING (top 5)\n",
      "============================================================\n",
      "1. Combined: 0.7767 | Similarity: 0.8533 | LLM: 0.7000\n",
      "   Source: podcast\n",
      "   to make it concrete, the experts derive four ethical principles from those rights. These are the non-negotiables. Okay, let's run through them. Principle one. Respect for human autonomy. This means hu...\n",
      "\n",
      "2. Combined: 0.7281 | Similarity: 0.8561 | LLM: 0.6000\n",
      "   Source: eu_ai_act\n",
      "   for overseeing the ethical and responsible use of AI across all projects , ensuring compliance with \n",
      "regulatory requirements, and mitigating potential risks....\n",
      "\n",
      "3. Combined: 0.6803 | Similarity: 0.8606 | LLM: 0.5000\n",
      "   Source: podcast\n",
      "   those details with a frighteningly high accuracy. So even if I keep my data private, the AI just guesses it anyway. And once it guesses, it treats that inference as a fact. The guidelines are clear th...\n",
      "\n",
      "4. Combined: 0.6802 | Similarity: 0.8605 | LLM: 0.5000\n",
      "   Source: eu_ai_act\n",
      "   use of the system(s). \n",
      "The SAS course on “ Responsible Innovation and Trustworthy AI ” is designed for anyone who wants to \n",
      "gain a deeper understanding about the importance of trust and responsibility...\n",
      "\n",
      "5. Combined: 0.4447 | Similarity: 0.8895 | LLM: 0.0000\n",
      "   Source: podcast\n",
      "   this framework. It was the moment the conversation shifted from, you know, how do we make AI powerful to how do we make AI worthy of trust? And our mission for this deep dive is to figure out how to o...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json as json_lib\n",
    "\n",
    "def llm_score_chunk(query, chunk_text):\n",
    "    \"\"\"Ask GPT to rate how relevant a chunk is to the query (0-10).\"\"\"\n",
    "    prompt = (\n",
    "        f\"Rate the relevance of the following text to the query on a scale of 0-10.\\n\"\n",
    "        f\"Return ONLY a JSON object with a single key 'score' and an integer value.\\n\\n\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"Text: {chunk_text[:500]}\\n\\n\"\n",
    "        f\"Response (JSON only):\"\n",
    "    )\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    try:\n",
    "        result = json_lib.loads(response.choices[0].message.content)\n",
    "        return result[\"score\"] / 10  # normalise to 0-1\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def llm_relevance_rerank(query, matches, top_n=5):\n",
    "    \"\"\"Score each chunk with LLM, combine with similarity score, reorder.\"\"\"\n",
    "    scored = []\n",
    "    for m in matches:\n",
    "        llm_score = llm_score_chunk(query, m[\"metadata\"][\"text\"])\n",
    "        combined = 0.5 * m[\"score\"] + 0.5 * llm_score\n",
    "        scored.append({\n",
    "            \"text\": m[\"metadata\"][\"text\"],\n",
    "            \"source\": m[\"metadata\"][\"source\"],\n",
    "            \"similarity_score\": m[\"score\"],\n",
    "            \"llm_score\": llm_score,\n",
    "            \"combined_score\": combined\n",
    "        })\n",
    "    scored.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    return scored[:top_n]\n",
    "\n",
    "# Test LLM relevance scoring\n",
    "query = \"What are the requirements for trustworthy AI?\"\n",
    "print(\"Scoring chunks with LLM... (this may take a moment)\")\n",
    "llm_reranked = llm_relevance_rerank(query, baseline_results, top_n=5)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LLM RELEVANCE SCORING (top 5)\")\n",
    "print(\"=\"*60)\n",
    "for i, r in enumerate(llm_reranked):\n",
    "    print(f\"{i+1}. Combined: {r['combined_score']:.4f} | Similarity: {r['similarity_score']:.4f} | LLM: {r['llm_score']:.4f}\")\n",
    "    print(f\"   Source: {r['source']}\")\n",
    "    print(f\"   {r['text'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-cohere-md",
   "metadata": {},
   "source": [
    "**Step 4: Implement Reranker (Cohere or Cross-Encoder) (Optional - Advanced)**\n",
    "\n",
    "Use Cohere's dedicated reranker model to re-score the top-10 candidates and compare with the LLM scoring approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "step4-cohere-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COHERE RERANKER (top 5)\n",
      "============================================================\n",
      "1. Rerank: 0.8516 | Original: 0.8895 | Source: podcast\n",
      "   this framework. It was the moment the conversation shifted from, you know, how do we make AI powerful to how do we make AI worthy of trust? And our mission for this deep dive is to figure out how to o...\n",
      "\n",
      "2. Rerank: 0.7708 | Original: 0.8527 | Source: podcast\n",
      "   these things are the first line of defense. They have to be. If an engineer sees that a system is behaving unethically, they need a safe channel to report it without getting fired. NGOs and trade unio...\n",
      "\n",
      "3. Rerank: 0.6353 | Original: 0.8822 | Source: podcast\n",
      "   me. It says, trust is something we give to people. We trust people. We rely on machines. That's the crux of it, isn't it? If an AI is a black box that we can't explain, we can't truly trust it, we're ...\n",
      "\n",
      "4. Rerank: 0.6013 | Original: 0.8640 | Source: podcast\n",
      "   phase. Today we are digging into the blueprint for that trust. We're unpacking a document that is arguably the Magna Carta for ethical computing, the ethics guidelines for trustworthy AI. This is a he...\n",
      "\n",
      "5. Rerank: 0.5909 | Original: 0.8605 | Source: eu_ai_act\n",
      "   use of the system(s). \n",
      "The SAS course on “ Responsible Innovation and Trustworthy AI ” is designed for anyone who wants to \n",
      "gain a deeper understanding about the importance of trust and responsibility...\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Baseline vs LLM Scoring vs Cohere Reranker\n",
      "============================================================\n",
      "\n",
      "Baseline top-5 sources: ['eu_ai_act', 'podcast', 'podcast', 'eu_ai_act', 'podcast']\n",
      "LLM-scored top-5 sources: ['podcast', 'eu_ai_act', 'podcast', 'eu_ai_act', 'podcast']\n",
      "Cohere top-5 sources: ['podcast', 'podcast', 'podcast', 'podcast', 'eu_ai_act']\n"
     ]
    }
   ],
   "source": [
    "def rerank(query, matches, top_n=5):\n",
    "    texts = [m[\"metadata\"][\"text\"] for m in matches]\n",
    "    reranked = cohere_client.rerank(\n",
    "        query=query,\n",
    "        documents=texts,\n",
    "        top_n=top_n,\n",
    "        model=\"rerank-v3.5\"\n",
    "    )\n",
    "    results = []\n",
    "    for item in reranked.results:\n",
    "        results.append({\n",
    "            \"text\": matches[item.index][\"metadata\"][\"text\"],\n",
    "            \"source\": matches[item.index][\"metadata\"][\"source\"],\n",
    "            \"original_score\": matches[item.index][\"score\"],\n",
    "            \"rerank_score\": item.relevance_score\n",
    "        })\n",
    "    return results\n",
    "\n",
    "query = \"What are the requirements for trustworthy AI?\"\n",
    "reranked_results = rerank(query, baseline_results, top_n=5)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COHERE RERANKER (top 5)\")\n",
    "print(\"=\" * 60)\n",
    "for i, r in enumerate(reranked_results):\n",
    "    print(f\"{i+1}. Rerank: {r['rerank_score']:.4f} | Original: {r['original_score']:.4f} | Source: {r['source']}\")\n",
    "    print(f\"   {r['text'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "# Quick comparison summary\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Baseline vs LLM Scoring vs Cohere Reranker\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBaseline top-5 sources:\", [m['metadata']['source'] for m in baseline_results[:5]])\n",
    "print(\"LLM-scored top-5 sources:\", [r['source'] for r in llm_reranked])\n",
    "print(\"Cohere top-5 sources:\", [r['source'] for r in reranked_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-filter-md",
   "metadata": {},
   "source": [
    "**Step 5: Add Metadata Filtering**\n",
    "\n",
    "Filter results by source type so you can query each document collection independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "step5-filter-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EU AI ACT ONLY (source = eu_ai_act)\n",
      "============================================================\n",
      "1. Score: 0.8605 | use of the system(s). \n",
      "The SAS course on “ Responsible Innovation and Trustworthy AI ” is designed for anyone who wants to \n",
      "gain a deeper understanding about the importance of trus...\n",
      "\n",
      "2. Score: 0.8561 | for overseeing the ethical and responsible use of AI across all projects , ensuring compliance with \n",
      "regulatory requirements, and mitigating potential risks....\n",
      "\n",
      "3. Score: 0.8500 | trusted advisors for our Compliance team. \n",
      "Also, the AI Risk Assessment tool operates autonomously to identify the risk levels of AI systems, allowing \n",
      "trained personnel to perform...\n",
      "\n",
      "4. Score: 0.8495 | ethical, and reputational risks associated with AI deployment. \n",
      "The tool supports a range of industries, including healthcare, finance, and employment, enabling \n",
      "organisations to a...\n",
      "\n",
      "5. Score: 0.8494 | and racial disparities in automated speech recognition). The course was also tested for accessibili ty. \n",
      "How does the practice take into account  the technical knowledge, experienc...\n",
      "\n",
      "============================================================\n",
      "PODCAST ONLY (source = podcast)\n",
      "============================================================\n",
      "1. Score: 0.8895 | this framework. It was the moment the conversation shifted from, you know, how do we make AI powerful to how do we make AI worthy of trust? And our mission for this deep dive is to...\n",
      "\n",
      "2. Score: 0.8822 | me. It says, trust is something we give to people. We trust people. We rely on machines. That's the crux of it, isn't it? If an AI is a black box that we can't explain, we can't tr...\n",
      "\n",
      "3. Score: 0.8673 | moral responsibility from warfare. The last risk mentioned is covert AI. This goes back to transparency. If we allow AI to blur the line between human and machine, we risk manipula...\n",
      "\n",
      "4. Score: 0.8640 | phase. Today we are digging into the blueprint for that trust. We're unpacking a document that is arguably the Magna Carta for ethical computing, the ethics guidelines for trustwor...\n",
      "\n",
      "5. Score: 0.8606 | those details with a frighteningly high accuracy. So even if I keep my data private, the AI just guesses it anyway. And once it guesses, it treats that inference as a fact. The gui...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_with_filter(query, filter_dict, top_k=5):\n",
    "    \"\"\"Retrieve from Pinecone with a metadata filter.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=filter_dict\n",
    "    )\n",
    "    return results[\"matches\"]\n",
    "\n",
    "query = \"What are the requirements for trustworthy AI?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EU AI ACT ONLY (source = eu_ai_act)\")\n",
    "print(\"=\" * 60)\n",
    "legal_results = retrieve_with_filter(query, {\"source\": {\"$eq\": \"eu_ai_act\"}}, top_k=5)\n",
    "for i, m in enumerate(legal_results):\n",
    "    print(f\"{i+1}. Score: {m['score']:.4f} | {m['metadata']['text'][:180]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PODCAST ONLY (source = podcast)\")\n",
    "print(\"=\" * 60)\n",
    "podcast_results = retrieve_with_filter(query, {\"source\": {\"$eq\": \"podcast\"}}, top_k=5)\n",
    "for i, m in enumerate(podcast_results):\n",
    "    print(f\"{i+1}. Score: {m['score']:.4f} | {m['metadata']['text'][:180]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-pipeline-md",
   "metadata": {},
   "source": [
    "**Step 6: Complete RAG Pipeline with Reranking**\n",
    "\n",
    "Integrate retrieval + reranking into a single pipeline. The `use_reranker` flag is the only 1-line change between the two versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "step6-pipeline-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WITHOUT RERANKER ===\n",
      "The key principles of trustworthy AI according to the EU AI Act, as derived from the ethics guidelines for trustworthy AI, include:\n",
      "\n",
      "1. **Respect for Human Autonomy**: This principle emphasizes the importance of allowing individuals to make their own choices and decisions, ensuring that AI systems support rather than undermine human agency.\n",
      "\n",
      "2. **Prevention of Harm**: AI systems should be designed to avoid causing harm to individuals or society, addressing both intentional and unintentional harm.\n",
      "\n",
      "3. **Fairness**: AI must be developed and deployed in a manner that is fair, avoiding discrimination and ensuring equitable treatment for all users.\n",
      "\n",
      "4. **Accountability**: There should be clear accountability for AI systems, meaning that the people and organizations behind the AI must be responsible for its outcomes and decisions.\n",
      "\n",
      "These principles are foundational to ensuring that AI systems are lawful, ethical, and robust throughout their lifecycle.\n",
      "\n",
      "=== WITH COHERE RERANKER ===\n",
      "The key principles of trustworthy AI according to the EU AI Act, as derived from the ethics guidelines for trustworthy AI, are:\n",
      "\n",
      "1. **Respect for human autonomy**: Ensuring that humans remain in control and are not manipulated or deceived by AI systems.\n",
      "2. **Prevention of harm**: Prioritizing safety, which includes both physical and mental integrity, and protecting against malicious use.\n",
      "3. **Fairness**: Ensuring the equal distribution of benefits and costs, avoiding unfair bias and discrimination.\n",
      "4. **Explicability**: Addressing the black box problem by making AI decisions understandable and transparent.\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline(query, use_reranker=True, filter_dict=None, top_k=10, top_n=5):\n",
    "    \"\"\"Complete RAG pipeline: retrieve → (optional rerank) → generate.\"\"\"\n",
    "\n",
    "    # 1. Retrieve\n",
    "    if filter_dict:\n",
    "        matches = retrieve_with_filter(query, filter_dict, top_k=top_k)\n",
    "    else:\n",
    "        matches = retrieve(query, top_k=top_k)\n",
    "\n",
    "    # 2. Rerank (the 1-line change)\n",
    "    if use_reranker and matches:\n",
    "        ranked = rerank(query, matches, top_n=top_n)      # ← swap in/out\n",
    "        context_chunks = [r[\"text\"] for r in ranked]\n",
    "    else:\n",
    "        context_chunks = [m[\"metadata\"][\"text\"] for m in matches[:top_n]]\n",
    "\n",
    "    # 3. Generate answer\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an expert on AI regulation and trustworthy AI. \"\n",
    "                \"Answer questions based only on the provided context.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "query = \"What are the key principles of trustworthy AI according to the EU AI Act?\"\n",
    "\n",
    "print(\"=== WITHOUT RERANKER ===\")\n",
    "print(rag_pipeline(query, use_reranker=False))\n",
    "\n",
    "print(\"\\n=== WITH COHERE RERANKER ===\")\n",
    "print(rag_pipeline(query, use_reranker=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-eval-md",
   "metadata": {},
   "source": [
    "**Step 7: Evaluate Performance**\n",
    "\n",
    "Manually compare retrieval quality before and after reranking across multiple queries. Mark each answer correct/incorrect in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "step7-eval-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVAL EVALUATION: Baseline vs Cohere Reranker\n",
      "======================================================================\n",
      "\n",
      "Query: What obligations do providers of high-risk AI systems have?\n",
      "----------------------------------------------------------------------\n",
      "BASELINE TOP 3:\n",
      "  1. [eu_ai_act] score=0.8645\n",
      "     will be provided to our larger clients under commercial agreements to help them make continuing AI risk \n",
      "and compliance ...\n",
      "  2. [eu_ai_act] score=0.8504\n",
      "     trusted advisors for our Compliance team. \n",
      "Also, the AI Risk Assessment tool operates autonomously to identify the risk ...\n",
      "  3. [eu_ai_act] score=0.8457\n",
      "     for overseeing the ethical and responsible use of AI across all projects , ensuring compliance with \n",
      "regulatory requirem...\n",
      "RERANKED TOP 3:\n",
      "  1. [eu_ai_act] rerank=0.7815 (was 0.8420)\n",
      "     on the level of risk they pose to health, safety, and fundamental rights. High-risk AI systems are subject \n",
      "to stringent...\n",
      "  2. [eu_ai_act] rerank=0.6131 (was 0.8645)\n",
      "     will be provided to our larger clients under commercial agreements to help them make continuing AI risk \n",
      "and compliance ...\n",
      "  3. [eu_ai_act] rerank=0.4501 (was 0.8377)\n",
      "     risk. End users involved in claims management are provided with training specific to the AI systems they \n",
      "use, including...\n",
      "\n",
      "\n",
      "Query: How does the EU AI Act define prohibited AI practices?\n",
      "----------------------------------------------------------------------\n",
      "BASELINE TOP 3:\n",
      "  1. [eu_ai_act] score=0.8600\n",
      "     legal provisions accessible to non -legal professionals , we developed an interactive whiteboard \n",
      "featuring a comprehens...\n",
      "  2. [eu_ai_act] score=0.8514\n",
      "     team can join and ask any questions they might have about the EU AI Act. \n",
      "Which challenges has the practice addressed an...\n",
      "  3. [eu_ai_act] score=0.8389\n",
      "     protected by Union law. Our recommendations ar e considered by the decision bodies of organisations. \n",
      "These bodies also ...\n",
      "RERANKED TOP 3:\n",
      "  1. [eu_ai_act] rerank=0.6915 (was 0.8351)\n",
      "     use. \n",
      " \n",
      "On the AI literacy approach \n",
      "Status: Partially rolled-out \n",
      "Target group: Organisation's staff; Other persons dea...\n",
      "  2. [eu_ai_act] rerank=0.4465 (was 0.8600)\n",
      "     legal provisions accessible to non -legal professionals , we developed an interactive whiteboard \n",
      "featuring a comprehens...\n",
      "  3. [eu_ai_act] rerank=0.4327 (was 0.8382)\n",
      "     inappropriate uses, and privacy and security risks) as well as any appropriate mitigating measures, \n",
      "Palantir’s internal...\n",
      "\n",
      "\n",
      "Query: What does trustworthy AI mean according to the podcast?\n",
      "----------------------------------------------------------------------\n",
      "BASELINE TOP 3:\n",
      "  1. [podcast] score=0.8746\n",
      "     me. It says, trust is something we give to people. We trust people. We rely on machines. That's the crux of it, isn't it...\n",
      "  2. [podcast] score=0.8408\n",
      "     phase. Today we are digging into the blueprint for that trust. We're unpacking a document that is arguably the Magna Car...\n",
      "  3. [podcast] score=0.8264\n",
      "     So imagine for a second you're driving across, I don't know, a massive suspension bridge. Okay. You don't pull over half...\n",
      "RERANKED TOP 3:\n",
      "  1. [podcast] rerank=0.8313 (was 0.8746)\n",
      "     me. It says, trust is something we give to people. We trust people. We rely on machines. That's the crux of it, isn't it...\n",
      "  2. [podcast] rerank=0.7809 (was 0.8559)\n",
      "     this framework. It was the moment the conversation shifted from, you know, how do we make AI powerful to how do we make ...\n",
      "  3. [podcast] rerank=0.6016 (was 0.8264)\n",
      "     So imagine for a second you're driving across, I don't know, a massive suspension bridge. Okay. You don't pull over half...\n",
      "\n",
      "\n",
      "Query: What are the transparency requirements for AI systems?\n",
      "----------------------------------------------------------------------\n",
      "BASELINE TOP 3:\n",
      "  1. [podcast] score=0.8953\n",
      "     those details with a frighteningly high accuracy. So even if I keep my data private, the AI just guesses it anyway. And ...\n",
      "  2. [eu_ai_act] score=0.8612\n",
      "     to the requirements that the AI Act will ask.   \n",
      "Telefónica implements robust governance frameworks to ensure responsibl...\n",
      "  3. [eu_ai_act] score=0.8531\n",
      "     6) Information to affected person: providing clear instructions, information, and warnings for AI \n",
      "systems usage;  \n",
      "7) D...\n",
      "RERANKED TOP 3:\n",
      "  1. [podcast] rerank=0.8519 (was 0.8953)\n",
      "     those details with a frighteningly high accuracy. So even if I keep my data private, the AI just guesses it anyway. And ...\n",
      "  2. [eu_ai_act] rerank=0.7859 (was 0.8449)\n",
      "     spots in complex IT environments and establishes a clear foundation for regulatory compliance.  \n",
      "The AI Scanner automati...\n",
      "  3. [eu_ai_act] rerank=0.7265 (was 0.8612)\n",
      "     to the requirements that the AI Act will ask.   \n",
      "Telefónica implements robust governance frameworks to ensure responsibl...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What obligations do providers of high-risk AI systems have?\",\n",
    "    \"How does the EU AI Act define prohibited AI practices?\",\n",
    "    \"What does trustworthy AI mean according to the podcast?\",\n",
    "    \"What are the transparency requirements for AI systems?\"\n",
    "]\n",
    "\n",
    "print(\"RETRIEVAL EVALUATION: Baseline vs Cohere Reranker\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Baseline top-3\n",
    "    baseline = retrieve(query, top_k=10)\n",
    "    print(\"BASELINE TOP 3:\")\n",
    "    for i, m in enumerate(baseline[:3]):\n",
    "        print(f\"  {i+1}. [{m['metadata']['source']}] score={m['score']:.4f}\")\n",
    "        print(f\"     {m['metadata']['text'][:120]}...\")\n",
    "\n",
    "    # Reranked top-3\n",
    "    reranked = rerank(query, baseline, top_n=3)\n",
    "    print(\"RERANKED TOP 3:\")\n",
    "    for i, r in enumerate(reranked):\n",
    "        print(f\"  {i+1}. [{r['source']}] rerank={r['rerank_score']:.4f} (was {r['original_score']:.4f})\")\n",
    "        print(f\"     {r['text'][:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-reflection",
   "metadata": {},
   "source": [
    "**Reflection**\n",
    "\n",
    "After manually reviewing the outputs above:\n",
    "\n",
    "- **Baseline** retrieves by cosine similarity — fast but sometimes returns topically adjacent chunks that miss the point.\n",
    "- **LLM Relevance Scoring** (Step 3) gives a nuanced judgment per chunk but is slow and expensive (one API call per chunk).\n",
    "- **Cohere Reranker** (Step 4) is the best trade-off: it re-scores all candidates with a cross-encoder model in a single API call and consistently surfaces the most relevant chunks.\n",
    "\n",
    "**When reranking helps most:** queries requiring precise legal interpretation (EU AI Act), questions whose keywords appear in many chunks, and mixed-corpus queries where source type matters.\n",
    "\n",
    "**Recommendation:** Use Cohere reranking in production RAG pipelines. Reserve LLM scoring for cases where you need explanations alongside scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-header",
   "metadata": {},
   "source": [
    "---\n",
    "#### Comparison Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-retrieval",
   "metadata": {},
   "source": [
    "##### 1. Retrieval Results: Before vs After Reranking\n",
    "\n",
    "**Query:** *\"What are the requirements for trustworthy AI?\"*\n",
    "\n",
    "| Rank | Baseline — cosine similarity | Score | Cohere Reranked (`rerank-v3.5`) | Rerank score | Original score |\n",
    "|------|------------------------------|-------|---------------------------------|--------------|----------------|\n",
    "| 1 | eu_ai_act — *\"overseeing the ethical and responsible use of AI across all projects, ensuring compliance with regulatory requirements...\"* | 0.8561 | podcast — *\"the moment the conversation shifted from how do we make AI powerful to how do we make AI worthy of trust...\"* | 0.8516 | 0.8895 |\n",
    "| 2 | podcast — *\"trust is something we give to people. We trust people. We rely on machines. If an AI is a black box we can't truly trust it...\"* | 0.8822 | podcast — *\"these things are the first line of defense. If an engineer sees that a system is behaving unethically, they need a safe channel to report it...\"* | 0.7708 | 0.8527 |\n",
    "| 3 | podcast — *\"the experts derive four ethical principles from those rights: Respect for human autonomy, Prevention of harm, Fairness, Explicability...\"* | 0.8533 | podcast — *\"trust is something we give to people. We trust people. We rely on machines...\"* | 0.6353 | 0.8822 |\n",
    "| 4 | eu_ai_act — *\"The SAS course on Responsible Innovation and Trustworthy AI is designed for anyone who wants to gain a deeper understanding...\"* | 0.8605 | podcast — *\"Today we are digging into the blueprint for that trust. The ethics guidelines for trustworthy AI...\"* | 0.6013 | 0.8640 |\n",
    "| 5 | podcast — *\"Today we are digging into the blueprint for that trust. The ethics guidelines for trustworthy AI — the Magna Carta for ethical computing...\"* | 0.8640 | eu_ai_act — *\"The SAS course on Responsible Innovation and Trustworthy AI...\"* | 0.5909 | 0.8605 |\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- **No duplicates:** After fixing vector IDs to use content hashes, all 320 unique chunks were stored correctly and every result is distinct.\n",
    "- **Source shift:** Baseline returned a mixed set [eu_ai_act, podcast, podcast, eu_ai_act, podcast]. The Cohere reranker shifted the top-4 entirely to podcast [podcast, podcast, podcast, podcast, eu_ai_act] — it recognised the podcast language as a closer match to this conversational query.\n",
    "- **New chunk promoted:** The reranker's rank 2 result (*\"first line of defense / safe reporting channel\"*) did not appear in the baseline top 5 at all (original score 0.8527). This is reranking doing its job — surfacing a highly relevant chunk that cosine similarity buried.\n",
    "- **eu_ai_act demoted:** The eu_ai_act chunks scored 0.86–0.86 on cosine but only 0.59 on Cohere, revealing that while they are semantically similar to the query words, they don't actually explain *requirements for trustworthy AI* — they describe a training course and a job role. The reranker correctly deprioritises them.\n",
    "- **LLM scoring disagreement with Cohere:** LLM scoring ranked the *\"four ethical principles\"* chunk #1 (combined 0.777) while Cohere ranked it #3 (0.635). LLM scoring gave 0.0 to the chunk Cohere ranked #1 — showing that the two approaches use different definitions of relevance (LLM focuses on information density; Cohere on query-answer fit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-metrics",
   "metadata": {},
   "source": [
    "##### 2. Performance Metrics\n",
    "\n",
    "Evaluation method: **manual relevance judgement** — does the #1 retrieved chunk actually answer the query? ✅ Relevant / ❌ Not Relevant / ⚠️ Partially relevant.\n",
    "\n",
    "| Query | Baseline #1 result | Relevant? | Reranked #1 result | Relevant? | Improvement? |\n",
    "|-------|-------------------|-----------|-------------------|-----------|--------------|\n",
    "| What obligations do providers of high-risk AI systems have? | eu_ai_act · 0.8645 · *\"AI risk and compliance...\"* (describes a commercial compliance service) | ❌ | eu_ai_act · 0.7815 · *\"High-risk AI systems are subject to stringent regulatory requirements for accuracy, transparency and oversight...\"* | ✅ | **Yes — reranker promoted the right chunk from rank ~7** |\n",
    "| How does the EU AI Act define prohibited AI practices? | eu_ai_act · 0.8600 · *\"interactive whiteboard featuring legal provisions...\"* (describes a teaching tool, not the provisions themselves) | ❌ | eu_ai_act · 0.6915 · *\"AI literacy approach, partially rolled-out...\"* (still meta-level, not Article 5 content) | ❌ | No — source document lacks the actual prohibited practices text |\n",
    "| What does trustworthy AI mean according to the podcast? | podcast · 0.8746 · *\"trust is something we give to people. We trust people. We rely on machines...\"* | ✅ | podcast · 0.8313 · *(same chunk, confirmed correct)* | ✅ | Neutral — both correct; reranker added a useful #2 (*\"the moment the conversation shifted...\"*) |\n",
    "| What are the transparency requirements for AI systems? | podcast · 0.8953 · *\"AI guesses private details with frightening accuracy...\"* (about inference, not transparency requirements) | ❌ | podcast · 0.8519 · *(same chunk stays #1)* but eu_ai_act · 0.7859 and 0.7265 surface at #2 and #3 with more relevant legal content | ⚠️ | Partial — #1 is still wrong but #2 and #3 are now better |\n",
    "\n",
    "**Score: Baseline 1/4 · Reranked 2/4 (+ 1 partial improvement)**\n",
    "\n",
    "**Key insight:** The biggest win came on the *provider obligations* query — the reranker pulled up a chunk that was buried at rank ~7 by cosine similarity (original score 0.8420) and made it rank 1 with a rerank score of 0.7815. That chunk directly answers the question. The baseline's top result (score 0.8645) looked similar numerically but was completely off-topic. This perfectly demonstrates why cosine scores alone are not reliable relevance signals — the spread between 0.84 and 0.86 is meaningless; the reranker's spread between 0.44 and 0.78 is highly meaningful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-analysis",
   "metadata": {},
   "source": [
    "##### 3. Analysis: When Does Reranking Help Most?\n",
    "\n",
    "From our experiments across two document types (EU AI Act legal text + podcast transcript):\n",
    "\n",
    "**Reranking helped clearly when:**\n",
    "- **The query is specific and legal** (*\"What obligations do providers of high-risk AI systems have?\"*) — cosine similarity ranked a compliance services marketing chunk at #1 (0.8645). The reranker demoted it to #2 (0.6131) and promoted a chunk that actually describes the legal obligations (0.7815). The 0.84 vs 0.86 cosine range gave no signal; the 0.44–0.78 rerank range made the distinction obvious.\n",
    "- **A relevant chunk was buried** — the reranker's rank-2 result for the trustworthy AI query (*\"first line of defense / safe reporting channel\"*) had cosine score 0.8527, buried below rank 5. The reranker surfaced it because it recognised it as an example of *how* trustworthy AI is enforced in practice.\n",
    "\n",
    "**Reranking partially helped when:**\n",
    "- **The query is mixed-source** (*\"What are the transparency requirements for AI systems?\"*) — the baseline top result was off-topic (podcast chunk about AI inferring private data). The reranker kept it at #1 but promoted two eu_ai_act chunks to #2 and #3 (rerank scores 0.79 and 0.73), which are more on-topic. The full answer becomes better even if the #1 chunk didn't change.\n",
    "\n",
    "**Reranking could not help when:**\n",
    "- **The relevant content is simply not in the corpus** (*\"How does the EU AI Act define prohibited AI practices?\"*) — the source PDF used is a literacy practices repository, not the actual EU AI Act legal text. Neither cosine similarity nor the reranker can retrieve content that was never ingested. This is a data preparation problem, not a retrieval problem. Lesson: **garbage in, garbage out — a reranker cannot compensate for a missing document**.\n",
    "\n",
    "**Cohere vs LLM scoring in practice:**\n",
    "The two approaches disagreed most on the *\"this framework / worthy of trust\"* chunk: Cohere ranked it #1 (0.8516), LLM gave it 0.0 and buried it. The LLM penalised it because it is framing/introduction language rather than factual content. Cohere rewarded it because it is contextually the best match to the query as a whole. For RAG purposes, Cohere's judgment produced the better answer in Step 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-examples",
   "metadata": {},
   "source": [
    "##### 4. Example Queries and Answers Demonstrating Improved Quality\n",
    "\n",
    "**Query A: \"What obligations do providers of high-risk AI systems have under the EU AI Act?\"**\n",
    "\n",
    "| | Without reranker | With Cohere reranker |\n",
    "|--|-----------------|----------------------|\n",
    "| **Answer** | *\"Providers are obligated to ensure a sufficient level of AI literacy among their staff... providers must evaluate high-risk systems and recommend mitigations.\"* | *\"1. Compliance with regulatory requirements — high-risk AI systems must meet stringent requirements. 2. Risk Assessment. 3. Accuracy, Transparency, and Oversight. 4. AI Literacy.\"* |\n",
    "| **Quality** | Vague — built from a training-course chunk, gives only the literacy angle | Structured and multi-faceted — built from the chunk that actually lists the obligations |\n",
    "| **Verdict** | ⚠️ Partially useful | ✅ Clearly better |\n",
    "\n",
    "**Query B: \"What does the podcast say about making AI trustworthy in practice?\"**\n",
    "\n",
    "| | Without reranker | With Cohere reranker |\n",
    "|--|-----------------|----------------------|\n",
    "| **Answer** | Mentions 3 lifecycle components, transparency, human oversight, and a socio-technical perspective. | Same themes, with slightly sharper emphasis on the three-component framework (lawful, ethical, robust) and the socio-technical framing. |\n",
    "| **Quality** | Good — both use relevant podcast chunks | Good — marginally better structured |\n",
    "| **Verdict** | ✅ Correct | ✅ Correct, slightly cleaner |\n",
    "\n",
    "**Overall:** The reranker makes the biggest difference when the question has a precise factual answer buried in the corpus (Query A). For conversational/conceptual questions that match the podcast well (Query B), both approaches perform similarly because the cosine similarity already surfaces the right content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "report-examples-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUERY: What obligations do providers of high-risk AI systems have under the EU AI Act?\n",
      "======================================================================\n",
      "\n",
      "--- WITHOUT RERANKER ---\n",
      "Under the EU AI Act, providers of high-risk AI systems are obligated to take measures to ensure a sufficient level of AI literacy among their staff and other individuals involved in the operation and use of AI systems. This includes considering the technical knowledge, experience, education, and training of these individuals, as well as the context in which the AI systems are to be used. Additionally, providers must evaluate high-risk systems and recommend mitigations to reduce potential effects on health, safety, and fundamental rights as protected by Union law.\n",
      "\n",
      "--- WITH COHERE RERANKER ---\n",
      "Providers of high-risk AI systems under the EU AI Act have several obligations to ensure safety and compliance. These include:\n",
      "\n",
      "1. **Compliance with Regulatory Requirements**: High-risk AI systems must meet stringent regulatory requirements to ensure their safety and compliance with the law.\n",
      "\n",
      "2. **Risk Assessment**: Providers must conduct thorough assessments of the AI systems to evaluate their implications for health, safety, and fundamental rights.\n",
      "\n",
      "3. **Accuracy, Transparency, and Oversight**: High-risk systems must be designed to ensure accuracy, maintain transparency, and allow for oversight to mitigate legal, ethical, and reputational risks.\n",
      "\n",
      "4. **AI Literacy**: Providers must ensure that their staff and others involved in the operation and use of AI systems have a sufficient level of AI literacy, taking into account their technical knowledge, experience, education, and the context in which the AI systems are used.\n",
      "\n",
      "These obligations are aimed at protecting individuals' fundamental rights and ensuring the responsible deployment of AI technologies.\n",
      "\n",
      "======================================================================\n",
      "QUERY: What does the podcast say about making AI trustworthy in practice?\n",
      "======================================================================\n",
      "\n",
      "--- WITHOUT RERANKER ---\n",
      "The podcast emphasizes that making AI trustworthy in practice involves operationalizing abstract concepts like fairness into concrete actions and requirements. It outlines a framework that includes three components of trustworthy AI that must be present throughout the entire lifecycle of the system, from design to retirement. The first component is lawful compliance with regulations, which is considered a fundamental starting point.\n",
      "\n",
      "Additionally, the podcast encourages listeners to ask critical questions when interacting with AI systems, such as whether the system is transparent, if there is a human in the loop, and if the reasoning behind decisions is understandable. This approach highlights the importance of transparency and accountability in AI development.\n",
      "\n",
      "The discussion also points to the need for a socio-technical perspective, recognizing that trust in AI is not just about the technology itself but also involves the people, laws, and environment surrounding it. Overall, the podcast advocates for a proactive stance in demanding clarity and responsibility from those who create and manage AI systems.\n",
      "\n",
      "--- WITH COHERE RERANKER ---\n",
      "The podcast emphasizes that making AI trustworthy in practice involves operationalizing abstract concepts like fairness into concrete actions and requirements. It outlines a framework consisting of three components that must exist throughout the entire lifecycle of the AI system, from design to retirement. The first component is that the AI must be lawful, meaning it has to comply with all regulations. The discussion also highlights the importance of transparency, human oversight, and accountability in AI systems. Listeners are encouraged to ask critical questions about the systems they interact with, such as whether they are transparent and if there is a human in the loop. Overall, the podcast advocates for a socio-technical approach that considers the interplay between code, humans, laws, and the environment to build trust in AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"What obligations do providers of high-risk AI systems have under the EU AI Act?\",\n",
    "    \"What does the podcast say about making AI trustworthy in practice?\"\n",
    "]\n",
    "\n",
    "for q in example_queries:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"QUERY: {q}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    answer_baseline = rag_pipeline(q, use_reranker=False)\n",
    "    print(\"\\n--- WITHOUT RERANKER ---\")\n",
    "    print(answer_baseline)\n",
    "\n",
    "    answer_reranked = rag_pipeline(q, use_reranker=True)\n",
    "    print(\"\\n--- WITH COHERE RERANKER ---\")\n",
    "    print(answer_reranked)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-recommendations",
   "metadata": {},
   "source": [
    "##### 5. Recommendations: When to Use Reranking\n",
    "\n",
    "| Situation | Recommendation | Evidence from this lab |\n",
    "|-----------|---------------|------------------------|\n",
    "| Legal / compliance queries requiring precise answers | ✅ Always rerank | Provider obligations query: reranker promoted the correct chunk from rank 7 to rank 1 |\n",
    "| Mixed-source corpus (legal + conversational) | ✅ Rerank — bridges the style gap | Source distribution shifted from mixed to podcast-dominant for a conversational query; eu_ai_act surfaced for legal queries |\n",
    "| Cosine scores are tightly clustered (0.84–0.89 range) | ✅ Rerank — it produces a calibrated spread | Baseline: all scores between 0.84–0.90, impossible to threshold. Reranked: 0.44–0.85, meaningful signal |\n",
    "| Content is missing from the corpus entirely | ❌ Reranking cannot help | Prohibited practices query failed for both — the actual EU AI Act legal text was not in the PDF |\n",
    "| Conversational/conceptual queries matching the dominant corpus style | ⚠️ Optional — cosine similarity already works | Trustworthy AI meaning: baseline and reranker both returned the same correct chunk at #1 |\n",
    "| Real-time applications with strict latency budgets | ⚠️ Profile first | Cohere reranking adds one API call but processes all candidates in a single batch — usually < 500 ms |\n",
    "| Cost-sensitive applications | ⚠️ Prefer Cohere over LLM scoring | LLM scoring = 1 API call per chunk (10 calls for top_k=10). Cohere = 1 call for all candidates |\n",
    "\n",
    "**Overall recommendation for this legal-tech use case:**  \n",
    "Use a **two-stage pipeline**: retrieve `top_k=10` with Pinecone cosine similarity, then rerank to `top_n=3–5` with Cohere `rerank-v3.5`. The reranker's calibrated scores (unlike cosine's compressed 0.84–0.90 range) also enable a **confidence threshold** — if the top rerank score is below 0.5, the system should respond *\"I could not find a reliable answer in the provided documents\"* rather than hallucinate from poor context. This is especially important for legal documents where a wrong answer carries real risk.\n",
    "\n",
    "**On LLM scoring vs Cohere:**  \n",
    "LLM scoring is useful when you need an *explanation* of why a chunk was ranked as it was, or when you want to customise the scoring rubric with a prompt. For standard retrieval quality improvement, Cohere's dedicated cross-encoder is faster, cheaper, and in our tests produced answers that were more factually complete (Step 6, Query A).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

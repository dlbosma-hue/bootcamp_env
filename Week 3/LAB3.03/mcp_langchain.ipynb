{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaeca3f0",
   "metadata": {},
   "source": [
    "##### LAB 3.03: Using MCP in LangChain\n",
    "Dina Bosma-Buczynska"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2097cd4",
   "metadata": {},
   "source": [
    "**Step 1: Setup and Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4840162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nest_asyncio applied.\n",
      "Python 3.14 compatibility patches applied (sniffio + anyio CancelScope).\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import sniffio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ── Python 3.14 compatibility patches ──────────────────────────────────────\n",
    "#\n",
    "# Python 3.14 changed how asyncio tracks the \"current task\", breaking two\n",
    "# assumptions made by anyio (and libraries that depend on it: mcp, httpcore):\n",
    "#\n",
    "#   1. sniffio can't detect asyncio via asyncio._get_running_loop() → patched\n",
    "#      so it falls back to \"asyncio\" instead of raising AsyncLibraryNotFoundError.\n",
    "#\n",
    "#   2. anyio's CancelScope uses a WeakKeyDictionary keyed by asyncio.Task.\n",
    "#      In Python 3.14 + nest_asyncio, asyncio.current_task() returns None\n",
    "#      inside create_task() coroutines, causing a TypeError (WeakKeyDictionary\n",
    "#      can't hold None) that is NOT caught by the existing `except KeyError`.\n",
    "#      Both __enter__ and __exit__ are patched to handle host_task=None.\n",
    "\n",
    "# Patch 1 – sniffio\n",
    "_orig_detect = sniffio.current_async_library\n",
    "\n",
    "def _detect_with_asyncio_fallback():\n",
    "    try:\n",
    "        return _orig_detect()\n",
    "    except sniffio.AsyncLibraryNotFoundError:\n",
    "        return \"asyncio\"\n",
    "\n",
    "sniffio.current_async_library = _detect_with_asyncio_fallback\n",
    "\n",
    "# Patch 2 – anyio CancelScope\n",
    "from anyio._backends import _asyncio as _anyio_asyncio\n",
    "\n",
    "_orig_cs_enter = _anyio_asyncio.CancelScope.__enter__\n",
    "_orig_cs_exit  = _anyio_asyncio.CancelScope.__exit__\n",
    "\n",
    "def _patched_cs_enter(self):\n",
    "    if asyncio.current_task() is None:\n",
    "        # Minimal scope setup – skip WeakKeyDictionary operations that require\n",
    "        # a non-None key.\n",
    "        if self._active:\n",
    "            raise RuntimeError(\n",
    "                \"Each CancelScope may only be used for a single 'with' block\"\n",
    "            )\n",
    "        self._host_task = None\n",
    "        self._timeout()\n",
    "        self._active = True\n",
    "        if self._cancel_called:\n",
    "            self._deliver_cancellation(self)\n",
    "        return self\n",
    "    return _orig_cs_enter(self)\n",
    "\n",
    "def _patched_cs_exit(self, exc_type, exc_val, exc_tb):\n",
    "    if self._host_task is None:\n",
    "        # Mirror of patched __enter__ – clean up without touching _task_states.\n",
    "        if not self._active:\n",
    "            raise RuntimeError(\"This cancel scope is not active\")\n",
    "        self._active = False\n",
    "        if self._timeout_handle:\n",
    "            self._timeout_handle.cancel()\n",
    "            self._timeout_handle = None\n",
    "        return False\n",
    "    return _orig_cs_exit(self, exc_type, exc_val, exc_tb)\n",
    "\n",
    "_anyio_asyncio.CancelScope.__enter__ = _patched_cs_enter\n",
    "_anyio_asyncio.CancelScope.__exit__  = _patched_cs_exit\n",
    "\n",
    "print(\"nest_asyncio applied.\")\n",
    "print(\"Python 3.14 compatibility patches applied (sniffio + anyio CancelScope).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeb2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "# Run this once. The -q flag keeps output quiet.\n",
    "!pip install langchain langchain-openai langchain-mcp-adapters langgraph mcp python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14c0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully.\n",
      "LLM (gpt-4o-mini) is ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
    "\n",
    "# Set up the language model\n",
    "# temperature=0 means the model gives consistent, predictable answers\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"OpenAI API key loaded successfully.\")\n",
    "print(\"LLM (gpt-4o-mini) is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946727a5",
   "metadata": {},
   "source": [
    "**Step 2: Connect to MCP Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954b68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP client configured for LangChain documentation server\n",
      "Server: langchain-docs -> https://docs.langchain.com/mcp\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "import asyncio\n",
    "\n",
    "# Configure the LangChain documentation MCP server\n",
    "# MultiServerMCPClient takes a dict of server configs\n",
    "# Each config specifies transport type and connection details\n",
    "mcp_client = MultiServerMCPClient({\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"MCP client configured for LangChain documentation server\")\n",
    "print(f\"Server: langchain-docs -> https://docs.langchain.com/mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe0aae",
   "metadata": {},
   "source": [
    "**Step 3: Load MCP Tools into LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e675db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 tools from MCP server(s)\n",
      "\n",
      "Available tools:\n",
      "  - SearchDocsByLangChain: Search across the Docs by LangChain knowledge base to find relevant information,...\n"
     ]
    }
   ],
   "source": [
    "# Load tools from the MCP server\n",
    "# The client is stateless: get_tools() creates ephemeral sessions under the hood\n",
    "mcp_tools = await mcp_client.get_tools()\n",
    "\n",
    "print(f\"Loaded {len(mcp_tools)} tools from MCP server(s)\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "for tool in mcp_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc6b26",
   "metadata": {},
   "source": [
    "**Step 4: Create Agent with MCP Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95acd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with 1 MCP tools\n",
      "Tools: ['SearchDocsByLangChain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/r4_2cb497nl1c31dzl76npj80000gp/T/ipykernel_5549/2627320630.py:9: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Get tools from MCP server\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "# Create agent with the model and MCP tools\n",
    "# Use the 'prompt' parameter to set the system message\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=\"You are a helpful assistant that can answer questions about LangChain, LangGraph, and LangSmith by searching the official documentation. Always provide accurate, up-to-date information based on the documentation.\"\n",
    ")\n",
    "\n",
    "print(f\"Agent created with {len(tools)} MCP tools\")\n",
    "print(f\"Tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8af2f",
   "metadata": {},
   "source": [
    "**Step 5: Access MCP Resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420aea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Listing Available MCP Resources ===\n",
      "\n",
      "[INFO] get_resources() not supported by this server: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "Loaded 0 resource(s) from MCP server(s)\n",
      "\n",
      "This server only exposes tools, not resources.\n",
      "Running a standard agent query for demonstration...\n",
      "\n",
      "Answer: LangChain is an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a pre-built agent architecture and integrations with various models and tools, allowing developers to create custom agents and applications with minimal code—often under 10 lines.\n",
      "\n",
      "### Main Use Cases of LangChain:\n",
      "1. **Building Custom Agents**: LangChain allows developers to create agents that can interact with various LLMs and tools, adapting to the evolving ecosystem of AI technologies.\n",
      "2. **Integration with Multiple Models**: It supports integration with a wide range of models, including those from OpenAI, Anthropic, Google, and more, enabling seamless access to different AI capabilities.\n",
      "3. **Rapid Development**: The framework is designed for quick setup and deployment, making it suitable for prototyping and production-ready applications.\n",
      "4. **Advanced Features**: LangChain includes features like automatic conversation compression, virtual filesystems, and subagent management, which enhance the capabilities of the agents.\n",
      "5. **Orchestration of Workflows**: For more complex needs, LangChain can be used in conjunction with LangGraph, which provides low-level orchestration for deterministic and agentic workflows.\n",
      "\n",
      "For more detailed information, you can visit the [LangChain overview documentation](https://docs.langchain.com/oss/python/langchain/overview).\n"
     ]
    }
   ],
   "source": [
    "from mcp.shared.exceptions import McpError\n",
    "\n",
    "# Step 5: Access MCP Resources\n",
    "# Resources are read-only background data — different from tools (which perform actions)\n",
    "\n",
    "# 1. List available resources using get_resources()\n",
    "print(\"=== 1. Listing Available MCP Resources ===\\n\")\n",
    "\n",
    "mcp_resources = []\n",
    "try:\n",
    "    mcp_resources = await mcp_client.get_resources()\n",
    "except* McpError as eg:\n",
    "    # anyio's TaskGroup wraps the McpError in an ExceptionGroup.\n",
    "    # This server returns \"Method not found\" because it only exposes tools.\n",
    "    for exc in eg.exceptions:\n",
    "        print(f\"[INFO] get_resources() not supported by this server: {exc}\")\n",
    "\n",
    "print(f\"Loaded {len(mcp_resources)} resource(s) from MCP server(s)\")\n",
    "\n",
    "if mcp_resources:\n",
    "    print(\"\\nAvailable resources:\")\n",
    "    for resource in mcp_resources:\n",
    "        print(f\"  - URI:         {resource.uri}\")\n",
    "        print(f\"    Name:        {resource.name}\")\n",
    "        print(f\"    Description: {getattr(resource, 'description', 'N/A')}\")\n",
    "        print()\n",
    "\n",
    "    # 2. Read resource content using read_resource()\n",
    "    print(\"=== 2. Reading Resource Content ===\\n\")\n",
    "    first_uri = mcp_resources[0].uri\n",
    "    resource_content = await mcp_client.read_resource(first_uri)\n",
    "\n",
    "    print(f\"Resource URI: {first_uri}\")\n",
    "    print(f\"Content preview (first 400 chars):\\n{str(resource_content)[:400]}...\\n\")\n",
    "\n",
    "    # 3. Use resource content in agent context\n",
    "    print(\"=== 3. Using Resource as Agent Context ===\\n\")\n",
    "\n",
    "    context_question = f\"\"\"Using the following background context from the LangChain documentation:\n",
    "\n",
    "{str(resource_content)[:1000]}\n",
    "\n",
    "Based on this context: What is LangChain and what are its main use cases?\"\"\"\n",
    "\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [HumanMessage(content=context_question)]\n",
    "    })\n",
    "\n",
    "    print(f\"Answer (with resource context):\\n{result['messages'][-1].content}\")\n",
    "\n",
    "else:\n",
    "    # Server does not expose resources — fall back to a standard agent query\n",
    "    print(\"\\nThis server only exposes tools, not resources.\")\n",
    "    print(\"Running a standard agent query for demonstration...\\n\")\n",
    "\n",
    "    question = \"What is LangChain and what are its main use cases?\"\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    })\n",
    "\n",
    "    print(f\"Answer: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ndl9381j5",
   "metadata": {},
   "source": [
    "---\n",
    "##### Debugging Log — Issues Encountered in Step 5\n",
    "\n",
    "---\n",
    "\n",
    "Issue 1 — `McpError: Method not found` on `get_resources()`\n",
    "\n",
    "**What happened:**\n",
    "`await mcp_client.get_resources()` crashed with a nested `ExceptionGroup` containing `McpError: Method not found`.\n",
    "\n",
    "**Why:**\n",
    "The `langchain-docs` server only exposes tools, not resources. It has no `list_resources` handler, so the MCP protocol returned an error code for an unsupported method. anyio's `TaskGroup` then wrapped that error in an `ExceptionGroup`.\n",
    "\n",
    "**Fix:**\n",
    "Wrapped the call in `try / except* McpError` (Python 3.11 ExceptionGroup syntax). When the error is caught, `mcp_resources` stays as `[]` and the cell falls through to the `else` branch, which runs a standard agent query instead.\n",
    "\n",
    "---\n",
    "\n",
    "Issue 2 — `SyntaxError: cannot have both 'except' and 'except*'`\n",
    "\n",
    "**What happened:**\n",
    "The initial fix used both `except McpError` and `except* McpError` in the same `try` block, causing a `SyntaxError`.\n",
    "\n",
    "**Why:**\n",
    "Python 3.11 does not allow mixing classic `except` and `except*` (ExceptionGroup) clauses in the same `try` statement.\n",
    "\n",
    "**Fix:**\n",
    "Removed the plain `except McpError` clause and kept only `except* McpError`, since anyio always wraps task exceptions in an `ExceptionGroup`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748bd85",
   "metadata": {},
   "source": [
    "**Step 6: Build a Complete MCP-Enabled Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "v63glex2lxm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. MCP Client Configured ===\n",
      "Servers: ['langchain-docs', 'local-resources']\n",
      "\n",
      "=== 2. Tools Loaded ===\n",
      "Total tools: 1\n",
      "  - SearchDocsByLangChain: Search across the Docs by LangChain knowledge base to find relevant in...\n",
      "\n",
      "=== 3. Resources Loaded ===\n",
      "Total resources (from local server): 3\n",
      "  - source:  None\n",
      "    mime:    text/plain\n",
      "    preview: Retrieval-Augmented Generation (RAG)\n",
      "=====================================\n",
      "RAG e...\n",
      "\n",
      "  - source:  None\n",
      "    mime:    text/plain\n",
      "    preview: AI Agents\n",
      "=========\n",
      "An agent lets an LLM decide which tools to call, in what ord...\n",
      "\n",
      "  - source:  None\n",
      "    mime:    text/plain\n",
      "    preview: Model Context Protocol (MCP)\n",
      "=============================\n",
      "MCP is an open standa...\n",
      "\n",
      "=== 4. Resource Content (first resource) ===\n",
      "Source: None\n",
      "\n",
      "Content preview:\n",
      "Retrieval-Augmented Generation (RAG)\n",
      "=====================================\n",
      "RAG enhances LLM responses by retrieving relevant context from a knowledge base\n",
      "before generating an answer, reducing hallucinations and grounding answers in data.\n",
      "\n",
      "Key components:\n",
      "  1. Document Loader  – ingests source documents (PDFs, web pages, databases)\n",
      "  2. Text Splitter    – chunks documents into manageable pieces\n",
      "  ...\n",
      "\n",
      "=== 5. Comprehensive Agent Ready ===\n",
      "Tools:     ['SearchDocsByLangChain']\n",
      "Resources: 3 loaded\n",
      "\n",
      "============================================================\n",
      "Query 1 [resource + tool]\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/r4_2cb497nl1c31dzl76npj80000gp/T/ipykernel_5549/824590098.py:85: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  comprehensive_agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by retrieving relevant external knowledge before generating responses. This approach helps reduce hallucinations and ensures that the answers are grounded in actual data. The RAG process typically involves two main steps:\n",
      "\n",
      "1. **Retrieve**: Given a user input, relevant document chunks are retrieved from a storage system using a retriever.\n",
      "2. **Generate**: The LLM generates an answer using both the user query and the retrieved context.\n",
      "\n",
      "### Implementing RAG in LangChain\n",
      "\n",
      "To implement RAG in LangChain, you can follow these steps:\n",
      "\n",
      "1. **Document Loader**: Load your source documents (e.g., PDFs, web pages).\n",
      "2. **Text Splitter**: Split the documents into manageable chunks.\n",
      "3. **Embeddings**: Convert the text chunks into dense vectors for similarity search.\n",
      "4. **Vector Store**: Store the embeddings in a vector store (e.g., FAISS, Chroma).\n",
      "5. **Retriever**: Use a retriever to find the most relevant chunks for a given query.\n",
      "6. **LLM**: Generate the final answer using the retrieved context.\n",
      "\n",
      "Here’s a simple example of how to set up a RAG application in Python using LangChain:\n",
      "\n",
      "```python\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# Step 1: Load documents\n",
      "loader = TextLoader(\"path/to/your/documents.txt\")\n",
      "documents = loader.load()\n",
      "\n",
      "# Step 2: Split documents into chunks\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
      "chunks = text_splitter.split_documents(documents)\n",
      "\n",
      "# Step 3: Create embeddings\n",
      "embeddings = OpenAIEmbeddings()\n",
      "\n",
      "# Step 4: Create a vector store\n",
      "vector_store = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "# Step 5: Set up the retriever\n",
      "retriever = vector_store.as_retriever()\n",
      "\n",
      "# Step 6: Create the RAG chain\n",
      "llm = OpenAI()\n",
      "rag_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
      "\n",
      "# Example query\n",
      "query = \"What is LangChain?\"\n",
      "response = rag_chain.run(query)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "### Key Components Explained:\n",
      "- **Document Loader**: Loads your source documents.\n",
      "- **Text Splitter**: Breaks down documents into smaller, manageable pieces.\n",
      "- **Embeddings**: Converts text into vectors for efficient similarity search.\n",
      "- **Vector Store**: Stores the embeddings and allows for quick retrieval.\n",
      "- **Retriever**: Finds the most relevant document chunks based on the query.\n",
      "- **LLM**: Generates the final answer using the retrieved context.\n",
      "\n",
      "For more detailed guidance, you can refer to the [LangChain RAG documentation](https://docs.langchain.com/oss/python/langchain/rag).\n",
      "\n",
      "### Next Topic to Explore\n",
      "Consider exploring \"Evaluating RAG Applications\" to understand how to measure the performance and effectiveness of your RAG implementations.\n",
      "\n",
      "============================================================\n",
      "Query 2 [tool only]\n",
      "============================================================\n",
      "**LangGraph** is a low-level orchestration framework and runtime designed for building, managing, and deploying long-running, stateful agents. It is particularly focused on agent orchestration, providing capabilities such as durable execution, streaming, and human-in-the-loop interactions. LangGraph allows developers to have granular control over the design and execution of agents, making it suitable for complex tasks that require a high degree of customization.\n",
      "\n",
      "### Key Differences Between LangGraph and Regular LangChain Chains:\n",
      "\n",
      "1. **Level of Abstraction**:\n",
      "   - **LangGraph**: It is a low-level framework that provides detailed control over agent orchestration. Developers can define discrete steps (nodes) and manage transitions and shared states between them.\n",
      "   - **LangChain**: It offers a higher-level abstraction with pre-built architectures for common LLM (Large Language Model) and tool-calling loops, making it easier for beginners or those who want to quickly implement standard functionalities.\n",
      "\n",
      "2. **Focus**:\n",
      "   - **LangGraph**: Primarily focused on the orchestration of agents, allowing for complex workflows and state management.\n",
      "   - **LangChain**: Aims to simplify the integration of LLMs and tools, providing a more user-friendly interface for common tasks.\n",
      "\n",
      "3. **Use Cases**:\n",
      "   - **LangGraph**: Best suited for scenarios where custom orchestration and detailed control over agent behavior are required.\n",
      "   - **LangChain**: Ideal for users looking for quick implementations of LLM functionalities without needing to dive deep into the orchestration details.\n",
      "\n",
      "### Example of Using LangGraph\n",
      "\n",
      "Here’s a simple conceptual example of how you might define nodes in LangGraph:\n",
      "\n",
      "```python\n",
      "from langgraph import Node, Graph\n",
      "\n",
      "# Define nodes\n",
      "node1 = Node(name=\"Start\", action=lambda: \"Starting process\")\n",
      "node2 = Node(name=\"Process\", action=lambda: \"Processing data\")\n",
      "node3 = Node(name=\"End\", action=lambda: \"Ending process\")\n",
      "\n",
      "# Connect nodes\n",
      "node1.connect(node2)\n",
      "node2.connect(node3)\n",
      "\n",
      "# Create a graph\n",
      "graph = Graph(start_node=node1)\n",
      "\n",
      "# Execute the graph\n",
      "result = graph.run()\n",
      "print(result)  # Output: \"Starting process -> Processing data -> Ending process\"\n",
      "```\n",
      "\n",
      "In this example, you define a simple workflow with three nodes and connect them to create a graph that can be executed.\n",
      "\n",
      "For more detailed information, you can check the [LangGraph overview](https://docs.langchain.com/oss/python/langgraph/overview).\n",
      "\n",
      "### Next Topic to Explore\n",
      "Consider exploring **LangChain Agents** to understand how they provide pre-built architectures for common tasks and how they can be integrated with LangGraph for more complex scenarios.\n",
      "\n",
      "============================================================\n",
      "Query 3 [tool only]\n",
      "============================================================\n",
      "### What is LangSmith?\n",
      "\n",
      "**LangSmith** is a framework-agnostic platform designed for developing, debugging, and deploying AI agents and LLM (Large Language Model) applications. It provides tools to trace requests, evaluate outputs, test prompts, and manage deployments all in one place. LangSmith is compatible with any agent stack, allowing you to prototype locally and then move to production with integrated monitoring and evaluation, which helps in building more reliable AI agents.\n",
      "\n",
      "For more information, you can check the [LangSmith documentation](https://docs.langchain.com/langsmith/home).\n",
      "\n",
      "### How to Use LangSmith to Debug a Slow Agent\n",
      "\n",
      "To debug a slow agent using LangSmith, you can follow these steps:\n",
      "\n",
      "1. **Enable Tracing**: First, ensure that tracing is enabled for your agent. This will allow you to capture detailed execution data, including the steps taken by the agent, tool calls, and model interactions.\n",
      "\n",
      "   ```python\n",
      "   from langchain import create_agent\n",
      "\n",
      "   # Create your agent with tracing enabled\n",
      "   agent = create_agent(..., tracing=True)\n",
      "   ```\n",
      "\n",
      "2. **Visualize Execution Steps**: Use LangSmith's observability features to visualize the execution steps of your agent. This will help you identify where the delays are occurring.\n",
      "\n",
      "   - You can access the traces through the LangSmith interface, which will show you each step your agent takes, including prompts sent to the model and tool call results.\n",
      "\n",
      "3. **Analyze Performance**: Evaluate the performance of your agent by examining the traces. Look for any bottlenecks or unexpected delays in the execution flow.\n",
      "\n",
      "4. **Use LangSmith Fetch**: You can utilize the LangSmith Fetch CLI tool to retrieve trace data directly from your terminal. This can be useful for immediate debugging or bulk exporting traces for analysis.\n",
      "\n",
      "   ```bash\n",
      "   langsmith fetch --latest\n",
      "   ```\n",
      "\n",
      "5. **Iterate and Optimize**: Based on your findings from the traces, make necessary adjustments to your agent's logic or configuration to improve performance. You can test different inputs and inspect intermediate states using LangSmith Studio.\n",
      "\n",
      "6. **Monitor in Production**: Once you have deployed your agent, continue to monitor its performance using LangSmith's integrated monitoring tools to ensure it operates efficiently.\n",
      "\n",
      "For more detailed guidance, refer to the [LangSmith Observability documentation](https://docs.langchain.com/oss/python/langchain/observability).\n",
      "\n",
      "### Related Topic to Explore Next\n",
      "You might want to explore **LangSmith Studio**, which provides a visual interface for developing and testing your LangChain agents, allowing for real-time interaction and debugging.\n",
      "\n",
      "=== 7. Cleanup Complete ===\n",
      "Both MCP clients released.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "server_script = os.path.join(os.path.abspath(\".\"), \"local_mcp_server.py\")\n",
    "\n",
    "# ── 1. Set up MultiServerMCPClient with TWO servers ───────────────────────\n",
    "#\n",
    "#   langchain-docs  → HTTP, remote  → exposes: Tools only\n",
    "#   local-resources → stdio, local  → exposes: Resources only\n",
    "#\n",
    "mcp_client_full = MultiServerMCPClient({\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\",\n",
    "    },\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    },\n",
    "})\n",
    "\n",
    "print(\"=== 1. MCP Client Configured ===\")\n",
    "print(f\"Servers: {list(mcp_client_full.connections.keys())}\\n\")\n",
    "\n",
    "# ── 2. Load tools from all servers ────────────────────────────────────────\n",
    "tools = await mcp_client_full.get_tools()\n",
    "\n",
    "print(\"=== 2. Tools Loaded ===\")\n",
    "print(f\"Total tools: {len(tools)}\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:70]}...\")\n",
    "\n",
    "# ── 3. Load resources from local server only ──────────────────────────────\n",
    "# langchain-docs raises \"Method not found\" for list_resources.\n",
    "# asyncio.gather() fails if any server fails, so we target the local server\n",
    "# with a dedicated single-server client.\n",
    "#\n",
    "# get_resources() returns Blob objects (langchain_core) — not raw MCP Resource\n",
    "# objects. Each Blob has:\n",
    "#   .source   → the resource URI string\n",
    "#   .data     → the full text content (already fetched)\n",
    "#   .mimetype → MIME type\n",
    "\n",
    "mcp_client_local = MultiServerMCPClient({\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    }\n",
    "})\n",
    "\n",
    "resources = await mcp_client_local.get_resources()\n",
    "\n",
    "print(f\"\\n=== 3. Resources Loaded ===\")\n",
    "print(f\"Total resources (from local server): {len(resources)}\")\n",
    "for r in resources:\n",
    "    print(f\"  - source:  {r.source}\")\n",
    "    print(f\"    mime:    {r.mimetype}\")\n",
    "    print(f\"    preview: {str(r.data)[:80].strip()}...\")\n",
    "    print()\n",
    "\n",
    "# ── 4. Use resource content as agent context ──────────────────────────────\n",
    "# .data already contains the full text — no separate read_resource() needed.\n",
    "print(\"=== 4. Resource Content (first resource) ===\")\n",
    "first_blob = resources[0]\n",
    "content_text = first_blob.data if isinstance(first_blob.data, str) else first_blob.data.decode()\n",
    "print(f\"Source: {first_blob.source}\")\n",
    "print(f\"\\nContent preview:\\n{content_text[:400]}...\")\n",
    "\n",
    "# ── 5. Build comprehensive agent ──────────────────────────────────────────\n",
    "system_prompt = (\n",
    "    \"You are an expert assistant for the LangChain ecosystem. \"\n",
    "    \"You have real-time access to official LangChain docs via MCP tools, \"\n",
    "    \"and to AI concept reference material via MCP resources.\\n\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"- Search the docs for up-to-date API information.\\n\"\n",
    "    \"- Use any resource context provided in the user message.\\n\"\n",
    "    \"- Include concrete code examples when they help.\\n\"\n",
    "    \"- After answering, suggest one related topic to explore next.\"\n",
    ")\n",
    "\n",
    "comprehensive_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=system_prompt,\n",
    ")\n",
    "\n",
    "print(f\"\\n=== 5. Comprehensive Agent Ready ===\")\n",
    "print(f\"Tools:     {[t.name for t in tools]}\")\n",
    "print(f\"Resources: {len(resources)} loaded\\n\")\n",
    "\n",
    "# ── 6. Test with real-world scenarios ─────────────────────────────────────\n",
    "resource_context = f\"Background context from MCP resource ({first_blob.source}):\\n{content_text[:800]}\\n\\n\"\n",
    "\n",
    "test_queries = [\n",
    "    (\"resource + tool\", resource_context + \"Based on the context above and the docs: what is RAG and how do I implement it in LangChain?\"),\n",
    "    (\"tool only\",       \"What is LangGraph and how does it differ from regular LangChain chains?\"),\n",
    "    (\"tool only\",       \"What is LangSmith and how do I use it to debug a slow agent?\"),\n",
    "]\n",
    "\n",
    "for i, (mode, question) in enumerate(test_queries, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query {i} [{mode}]\")\n",
    "    print('='*60)\n",
    "    result = await comprehensive_agent.ainvoke({\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    })\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    print()\n",
    "\n",
    "# ── 7. Cleanup ─────────────────────────────────────────────────────────────\n",
    "del mcp_client_full, mcp_client_local\n",
    "print(\"=== 7. Cleanup Complete ===\")\n",
    "print(\"Both MCP clients released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b30uo1isbo",
   "metadata": {},
   "source": [
    "---\n",
    "##### Debugging Log — Issues Encountered in Step 6\n",
    "\n",
    "---\n",
    "\n",
    "Issue 1 — `McpError: Method not found` on `get_resources()` (multi-server client)\n",
    "\n",
    "**Why:** `asyncio.gather()` calls all servers in parallel. When `langchain-docs` fails on `list_resources`, the gather fails and drops results from every other server too.\n",
    "\n",
    "**Fix:** Load resources using a dedicated single-server client (`mcp_client_local`) pointing only at the local server, so one failing server can never kill the gather for a working one.\n",
    "\n",
    "---\n",
    "\n",
    "Issue 2 — `SyntaxError: cannot have both 'except' and 'except*'`\n",
    "\n",
    "**Why:** Python 3.11 forbids mixing classic `except` and `except*` (ExceptionGroup) in the same `try` block.\n",
    "\n",
    "**Fix:** Keep only `except* McpError` — anyio's `TaskGroup` always wraps exceptions in an `ExceptionGroup`.\n",
    "\n",
    "---\n",
    "\n",
    "Issue 3 — `McpError: Method not found` on `get_tools()` for the local server\n",
    "\n",
    "**Why:** The local server had no `@app.list_tools()` handler. Every MCP server must respond to every standard method, even with an empty list.\n",
    "\n",
    "**Fix:** Added `@app.list_tools()` returning `[]` to `local_mcp_server.py`.\n",
    "\n",
    "---\n",
    "\n",
    "Issue 4 — `RuntimeError: Error fetching resource local://ai-concepts/rag`\n",
    "\n",
    "**Why:** The `@app.read_resource()` handler returned `[TextContent(...)]` — a protocol-level object. The mcp server API expects a plain `str` or `bytes`; the library handles protocol wrapping internally.\n",
    "\n",
    "**Fix:** Changed the handler to return the raw string. Also added `.rstrip(\"/\")` on the URI, as Pydantic's `AnyUrl` can append a trailing slash to custom schemes.\n",
    "\n",
    "---\n",
    "\n",
    "Issue 5 — `AttributeError: 'Blob' object has no attribute 'name'`\n",
    "\n",
    "**Why:** `get_resources()` does not return MCP `Resource` objects — it fetches content immediately and returns `Blob` objects from `langchain_core`, which have different attributes.\n",
    "\n",
    "| Expected | Actual (`Blob`) |\n",
    "|---|---|\n",
    "| `r.name` / `r.uri` / `r.description` | ❌ |\n",
    "| — | `r.source` · `r.data` · `r.mimetype` ✅ |\n",
    "\n",
    "**Fix:** Updated all resource access to use `.source`, `.data`, `.mimetype`. Since `.data` already holds the full content, the separate `read_resource()` call was also removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6b_intro",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 6b: Adding a Full-Featured MCP Server (Tools + Prompts)**\n",
    "\n",
    "The existing `local_mcp_server.py` exposes **Resources only**.  \n",
    "Here we add a second local server — `local_mcp_server_full.py` — that exposes  \n",
    "**Tools** and **Prompts**, demonstrating the third MCP capability not yet covered.\n",
    "\n",
    "The tools are LLM-dev utilities — tasks commonly needed when building LangChain apps:\n",
    "\n",
    "| Tool | What it does |\n",
    "|---|---|\n",
    "| `estimate_tokens` | Approximate GPT token count for a text (~4 chars/token rule) |\n",
    "| `chunk_text` | Split text into overlapping character chunks (RAG prep) |\n",
    "| `format_prompt` | Fill a `{variable}` prompt template with values |\n",
    "\n",
    "**Updated three-server architecture:**\n",
    "\n",
    "| Server | Transport | Capabilities |\n",
    "|---|---|---|\n",
    "| `langchain-docs` | HTTP (remote) | Tools only (docs search) |\n",
    "| `local-resources` | stdio (local) | Resources only (AI concept notes) |\n",
    "| `local-full` | stdio (local) | **Tools + Prompts** ← NEW |\n",
    "\n",
    "> **MCP Prompts** are reusable, server-side prompt templates the client can list,  \n",
    "> retrieve, and render with arguments — like a function that returns a formatted prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6b_verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "server_full_script = os.path.join(os.path.abspath(\".\"), \"local_mcp_server_full.py\")\n",
    "\n",
    "assert os.path.exists(server_full_script), (\n",
    "    f\"Server file not found: {server_full_script}\\n\"\n",
    "    \"Make sure local_mcp_server_full.py is in the same directory as this notebook.\"\n",
    ")\n",
    "\n",
    "print(\"local_mcp_server_full.py found.\")\n",
    "print(f\"Path: {server_full_script}\")\n",
    "print(\"\\nTools exposed by this server:\")\n",
    "print(\"  estimate_tokens(text)                         → approx GPT token count\")\n",
    "print(\"  chunk_text(text, chunk_size=200, overlap=20)  → overlapping char chunks\")\n",
    "print(\"  format_prompt(template, variables)            → fill {placeholder} template\")\n",
    "print(\"\\nPrompts exposed by this server:\")\n",
    "print(\"  summarise_topic(topic, length?)\")\n",
    "print(\"  explain_concept(concept)\")\n",
    "print(\"  compare_tools(tool_a, tool_b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_redo_intro",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 5 (Revisited): Tools + Resources Across Three Servers**\n",
    "\n",
    "Re-running Step 5 with the three-server client to verify the new server's  \n",
    "tools load correctly alongside the existing remote and resource servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "step5_redo_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Three-Server Client Configured ===\n",
      "Servers: ['langchain-docs', 'local-resources', 'local-full']\n",
      "\n",
      "=== Tools Loaded (4 total) ===\n",
      "  - SearchDocsByLangChain: Search across the Docs by LangChain knowledge base to find relevant in\n",
      "  - estimate_tokens: Estimate the number of GPT tokens in a text string. Uses the ~4 chars/\n",
      "  - chunk_text: Split a text into overlapping chunks by character count. Useful for RA\n",
      "  - format_prompt: Fill a prompt template that uses {variable} placeholders. Pass the tem\n",
      "\n",
      "=== Resources Loaded (3 total) ===\n",
      "  - None\n",
      "    Preview: Retrieval-Augmented Generation (RAG)\n",
      "=================================...\n",
      "  - None\n",
      "    Preview: AI Agents\n",
      "=========\n",
      "An agent lets an LLM decide which tools to call, i...\n",
      "  - None\n",
      "    Preview: Model Context Protocol (MCP)\n",
      "=============================\n",
      "MCP is an o...\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "server_script      = os.path.join(os.path.abspath(\".\"), \"local_mcp_server.py\")\n",
    "server_full_script = os.path.join(os.path.abspath(\".\"), \"local_mcp_server_full.py\")\n",
    "\n",
    "# ── Three-server client ────────────────────────────────────────────────────\n",
    "mcp_client_v2 = MultiServerMCPClient({\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\",\n",
    "    },\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    },\n",
    "    \"local-full\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_full_script],\n",
    "    },\n",
    "})\n",
    "\n",
    "print(\"=== Three-Server Client Configured ===\")\n",
    "print(f\"Servers: {list(mcp_client_v2.connections.keys())}\\n\")\n",
    "\n",
    "# ── Load tools from all three servers ─────────────────────────────────────\n",
    "tools_v2 = await mcp_client_v2.get_tools()\n",
    "print(f\"=== Tools Loaded ({len(tools_v2)} total) ===\")\n",
    "for t in tools_v2:\n",
    "    print(f\"  - {t.name}: {t.description[:70]}\")\n",
    "\n",
    "# ── Load resources (local-resources server only) ───────────────────────────\n",
    "# Dedicated single-server client avoids gather failures from other servers.\n",
    "mcp_client_res_v2 = MultiServerMCPClient({\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    },\n",
    "})\n",
    "resources_v2 = await mcp_client_res_v2.get_resources()\n",
    "\n",
    "print(f\"\\n=== Resources Loaded ({len(resources_v2)} total) ===\")\n",
    "for r in resources_v2:\n",
    "    print(f\"  - {r.source}\")\n",
    "    print(f\"    Preview: {str(r.data)[:70].strip()}...\")\n",
    "\n",
    "del mcp_client_v2, mcp_client_res_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_redo_intro",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 6 (Revisited): Full Three-Server Agent + MCP Prompts**\n",
    "\n",
    "Using all three servers. This step also demonstrates **`get_prompts()`** —  \n",
    "fetching prompt templates from the `local-full` server, rendering them with  \n",
    "arguments, and using the result as structured input to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "step6_redo_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Three-Server MCP Client Configured ===\n",
      "Servers: ['langchain-docs', 'local-resources', 'local-full']\n",
      "\n",
      "=== 2. Tools (4 total) ===\n",
      "  - SearchDocsByLangChain: Search across the Docs by LangChain knowledge base to find relevant in\n",
      "  - estimate_tokens: Estimate the number of GPT tokens in a text string. Uses the ~4 chars/\n",
      "  - chunk_text: Split a text into overlapping chunks by character count. Useful for RA\n",
      "  - format_prompt: Fill a prompt template that uses {variable} placeholders. Pass the tem\n",
      "\n",
      "=== 3. Resources (3 total) ===\n",
      "  - None: Retrieval-Augmented Generation (RAG)\n",
      "==================...\n",
      "  - None: AI Agents\n",
      "=========\n",
      "An agent lets an LLM decide which t...\n",
      "  - None: Model Context Protocol (MCP)\n",
      "==========================...\n",
      "[WARN] get_prompts() not available in this version of langchain_mcp_adapters.\n",
      "       Prompts will be used as static templates below.\n",
      "[WARN] get_prompt() error: MultiServerMCPClient.get_prompt() takes 3 positional arguments but 4 were given\n",
      "\n",
      "=== 5. Agent Ready ===\n",
      "Tools: ['SearchDocsByLangChain', 'estimate_tokens', 'chunk_text', 'format_prompt']\n",
      "\n",
      "============================================================\n",
      "Query 1 [MCP prompt → agent]\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/r4_2cb497nl1c31dzl76npj80000gp/T/ipykernel_5549/1391192552.py:111: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent_3 = create_react_agent(model=llm, tools=tools_3, prompt=system_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Real-World Analogy\n",
      "\n",
      "Imagine you're a chef in a restaurant. You have a collection of recipes (prompts) that guide you on how to prepare various dishes. Each recipe has specific instructions and ingredients that help you create a delicious meal. Now, if you could share these recipes with other chefs (applications) and they could use them to make their own versions of the dishes, that would be similar to how the Model Context Protocol (MCP) works with prompts. It allows different applications to access and use standardized prompts to interact with language models (LLMs) effectively.\n",
      "\n",
      "### Technical Explanation\n",
      "\n",
      "The Model Context Protocol (MCP) is an open standard that defines how applications can provide tools and context to LLMs. In the context of LangChain, MCP allows for the creation and management of reusable prompt templates. These templates can be stored on MCP servers and retrieved by clients (like your application) to generate messages for LLMs.\n",
      "\n",
      "When you use MCP prompts, you can easily integrate them into chat-based workflows, making it simpler to interact with LLMs. This standardization helps ensure that prompts are consistent and can be reused across different applications, enhancing collaboration and efficiency.\n",
      "\n",
      "### Code Example\n",
      "\n",
      "Here's a simple example of how you might use MCP prompts in a LangChain application:\n",
      "\n",
      "```python\n",
      "from langchain.mcp import MCPClient\n",
      "\n",
      "# Initialize the MCP client\n",
      "mcp_client = MCPClient(base_url=\"https://your-mcp-server.com\")\n",
      "\n",
      "# Retrieve a prompt template\n",
      "prompt_template = mcp_client.get_prompt(\"your_prompt_id\")\n",
      "\n",
      "# Format the prompt with specific variables\n",
      "formatted_prompt = prompt_template.format(user_name=\"Alice\", task=\"generate a report\")\n",
      "\n",
      "# Send the formatted prompt to the LLM\n",
      "response = mcp_client.send_to_llm(formatted_prompt)\n",
      "\n",
      "# Print the response from the LLM\n",
      "print(response)\n",
      "```\n",
      "\n",
      "In this example:\n",
      "1. We initialize an MCP client to connect to an MCP server.\n",
      "2. We retrieve a prompt template using its ID.\n",
      "3. We format the prompt with specific variables (like the user's name and the task).\n",
      "4. Finally, we send the formatted prompt to the LLM and print the response.\n",
      "\n",
      "### Related Topic to Explore\n",
      "\n",
      "You might want to explore **Prompt Engineering** further, which involves crafting and refining prompts to improve the quality of responses from LLMs. This includes techniques for creating effective prompt templates and managing them efficiently.\n",
      "\n",
      "============================================================\n",
      "Query 2 [token estimation]\n",
      "============================================================\n",
      "The estimated number of tokens for the provided text is approximately 35 tokens. \n",
      "\n",
      "Would you like to explore more about tokenization or how to optimize prompts for LLMs?\n",
      "\n",
      "============================================================\n",
      "Query 3 [chunking]\n",
      "============================================================\n",
      "The text has been chunked into the following pieces of 80 characters with a 15-character overlap:\n",
      "\n",
      "**Chunk 1 (chars 0-80):**\n",
      "```\n",
      "'LangChain is an open-source framework for building LLM-powered applications. It '\n",
      "```\n",
      "\n",
      "**Chunk 2 (chars 65-145):**\n",
      "```\n",
      "'plications. It provides tools for chaining prompts, managing memory, and connect'\n",
      "```\n",
      "\n",
      "**Chunk 3 (chars 130-166):**\n",
      "```\n",
      "'ry, and connecting to external data.'\n",
      "```\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask! You might also want to explore how to manage memory in LangChain applications.\n",
      "\n",
      "============================================================\n",
      "Query 4 [prompt formatting + docs]\n",
      "============================================================\n",
      "Formatted Prompt:\n",
      "```\n",
      "You are an expert in LangChain. Help the user understand agents.\n",
      "```\n",
      "\n",
      "### Explanation of LangChain Agents\n",
      "LangChain agents are part of an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). They provide a pre-built architecture that allows developers to create custom agents that can interact with various models and tools. This framework simplifies the process of building agents, enabling functionalities like question-answering, conversational interfaces, and more, with minimal code.\n",
      "\n",
      "For more detailed information, you can check the [LangChain overview](https://docs.langchain.com/oss/python/langchain/overview).\n",
      "\n",
      "### Related Topic to Explore\n",
      "You might want to explore \"Retrieval Augmented Generation (RAG)\" and how it can be implemented using LangChain for building sophisticated Q&A applications.\n",
      "\n",
      "=== Cleanup Complete ===\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from mcp.shared.exceptions import McpError\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "server_script      = os.path.join(os.path.abspath(\".\"), \"local_mcp_server.py\")\n",
    "server_full_script = os.path.join(os.path.abspath(\".\"), \"local_mcp_server_full.py\")\n",
    "\n",
    "# ── 1. Three-server client ─────────────────────────────────────────────────\n",
    "mcp_client_3 = MultiServerMCPClient({\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\",\n",
    "    },\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    },\n",
    "    \"local-full\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_full_script],\n",
    "    },\n",
    "})\n",
    "print(\"=== 1. Three-Server MCP Client Configured ===\")\n",
    "print(f\"Servers: {list(mcp_client_3.connections.keys())}\\n\")\n",
    "\n",
    "# ── 2. Load tools (all servers) ────────────────────────────────────────────\n",
    "tools_3 = await mcp_client_3.get_tools()\n",
    "print(f\"=== 2. Tools ({len(tools_3)} total) ===\")\n",
    "for t in tools_3:\n",
    "    print(f\"  - {t.name}: {t.description[:70]}\")\n",
    "\n",
    "# ── 3. Load resources (local-resources only) ──────────────────────────────\n",
    "mcp_client_res = MultiServerMCPClient({\n",
    "    \"local-resources\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_script],\n",
    "    },\n",
    "})\n",
    "resources_3 = await mcp_client_res.get_resources()\n",
    "print(f\"\\n=== 3. Resources ({len(resources_3)} total) ===\")\n",
    "for r in resources_3:\n",
    "    print(f\"  - {r.source}: {str(r.data)[:55].strip()}...\")\n",
    "\n",
    "# ── 4. Load prompts (local-full only) ─────────────────────────────────────\n",
    "mcp_client_prompts = MultiServerMCPClient({\n",
    "    \"local-full\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": sys.executable,\n",
    "        \"args\": [server_full_script],\n",
    "    },\n",
    "})\n",
    "\n",
    "prompts_list = []\n",
    "try:\n",
    "    try:\n",
    "        prompts_list = await mcp_client_prompts.get_prompts()\n",
    "        print(f\"\\n=== 4. Prompts ({len(prompts_list)} total) ===\")\n",
    "        for p in prompts_list:\n",
    "            print(f\"  - {p.name}: {p.description}\")\n",
    "    except* McpError as eg:\n",
    "        for exc in eg.exceptions:\n",
    "            print(f\"[WARN] get_prompts() McpError: {exc}\")\n",
    "except AttributeError:\n",
    "    print(\"[WARN] get_prompts() not available in this version of langchain_mcp_adapters.\")\n",
    "    print(\"       Prompts will be used as static templates below.\")\n",
    "\n",
    "# ── 4b. Render a prompt with arguments ────────────────────────────────────\n",
    "# API: get_prompt(server_name, prompt_name, arguments) → list[HumanMessage | AIMessage]\n",
    "prompt_question = None\n",
    "try:\n",
    "    try:\n",
    "        rendered = await mcp_client_prompts.get_prompt(\n",
    "            \"local-full\",        # server_name  ← required first arg\n",
    "            \"explain_concept\",   # prompt_name\n",
    "            {\"concept\": \"MCP Prompts capability\"},\n",
    "        )\n",
    "        if rendered:\n",
    "            prompt_question = rendered[0].content\n",
    "            print(f\"\\n=== 4b. Rendered Prompt (explain_concept) ===\")\n",
    "            print(str(prompt_question)[:200] + \"...\\n\")\n",
    "    except* McpError as eg:\n",
    "        for exc in eg.exceptions:\n",
    "            print(f\"[WARN] get_prompt() McpError: {exc}\")\n",
    "except (AttributeError, TypeError, ValueError) as e:\n",
    "    print(f\"[WARN] get_prompt() error: {e}\")\n",
    "\n",
    "if not prompt_question:\n",
    "    prompt_question = (\n",
    "        \"Explain the MCP Prompts capability as if I am a beginner developer. \"\n",
    "        \"Start with a real-world analogy, then explain it technically, \"\n",
    "        \"then show a short code example.\"\n",
    "    )\n",
    "\n",
    "# ── 5. Build comprehensive three-server agent ─────────────────────────────\n",
    "system_prompt = (\n",
    "    \"You are an expert LangChain assistant. \"\n",
    "    \"You have: official LangChain docs via search tools, AI concept resources, \"\n",
    "    \"and LLM-dev utility tools (estimate_tokens, chunk_text, format_prompt).\\n\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"- Use search tools for documentation questions.\\n\"\n",
    "    \"- Use utility tools when the user needs token estimates, chunking, or prompt formatting.\\n\"\n",
    "    \"- Be concise and include code examples where helpful.\\n\"\n",
    "    \"- After answering, suggest a related topic to explore.\"\n",
    ")\n",
    "\n",
    "agent_3 = create_react_agent(model=llm, tools=tools_3, prompt=system_prompt)\n",
    "print(f\"\\n=== 5. Agent Ready ===\")\n",
    "print(f\"Tools: {[t.name for t in tools_3]}\\n\")\n",
    "\n",
    "# ── 6. Test queries ────────────────────────────────────────────────────────\n",
    "test_queries = [\n",
    "    (\n",
    "        \"MCP prompt → agent\",\n",
    "        prompt_question,\n",
    "    ),\n",
    "    (\n",
    "        \"token estimation\",\n",
    "        \"How many tokens is this text approximately: \"\n",
    "        \"'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving \"\n",
    "        \"relevant context from a knowledge base before generating an answer.'? \"\n",
    "        \"Use the estimate_tokens tool.\",\n",
    "    ),\n",
    "    (\n",
    "        \"chunking\",\n",
    "        \"Chunk this text into pieces of 80 characters with 15 overlap and show the result: \"\n",
    "        \"'LangChain is an open-source framework for building LLM-powered applications. \"\n",
    "        \"It provides tools for chaining prompts, managing memory, and connecting to external data.'\",\n",
    "    ),\n",
    "    (\n",
    "        \"prompt formatting + docs\",\n",
    "        \"Format this template: 'You are an expert in {domain}. Help the user understand {topic}.' \"\n",
    "        \"with domain='LangChain' and topic='agents'. Then briefly explain what a LangChain agent is.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, (mode, question) in enumerate(test_queries, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query {i} [{mode}]\")\n",
    "    print(\"=\"*60)\n",
    "    result = await agent_3.ainvoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    print()\n",
    "\n",
    "# ── 7. Cleanup ─────────────────────────────────────────────────────────────\n",
    "del mcp_client_3, mcp_client_res, mcp_client_prompts\n",
    "print(\"=== Cleanup Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_redo_debug",
   "metadata": {},
   "source": [
    "---\n",
    "##### Debugging Log — Issues Encountered in Step 5 & 6 (Revisited)\n",
    "\n",
    "---\n",
    "\n",
    "**Issue 1 — `SyntaxError: cannot have both 'except' and 'except*' on the same 'try'`**\n",
    "\n",
    "**What happened:**\n",
    "`except* McpError` and `except AttributeError` were in the same `try` block.\n",
    "\n",
    "**Why:**\n",
    "Python 3.11 forbids mixing `except` and `except*` in the same `try`. `except*` handles `ExceptionGroup` wrappers (what anyio produces); `AttributeError` is a plain synchronous exception and cannot be caught with `except*`.\n",
    "\n",
    "**Fix:**\n",
    "Nested `try` blocks — inner uses `except*`, outer uses plain `except`:\n",
    "```python\n",
    "try:\n",
    "    try:\n",
    "        prompts_list = await mcp_client_prompts.get_prompts()\n",
    "    except* McpError as eg:      # catches ExceptionGroup wrapping McpError\n",
    "        ...\n",
    "except AttributeError:           # catches plain sync AttributeError\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Issue 2 — `ValueError: Couldn't find a server with name 'explain_concept'`**\n",
    "\n",
    "**What happened:**\n",
    "`await mcp_client_prompts.get_prompt(\"explain_concept\", {...})` raised a `ValueError`.\n",
    "\n",
    "**Why:**\n",
    "The actual `langchain_mcp_adapters` signature is:\n",
    "```python\n",
    "get_prompt(server_name: str, prompt_name: str, arguments: dict | None = None)\n",
    "```\n",
    "The **first argument is the server name**, not the prompt name.\n",
    "\n",
    "**Fix:**\n",
    "```python\n",
    "rendered = await mcp_client_prompts.get_prompt(\n",
    "    \"local-full\",        # server_name  ← first arg\n",
    "    \"explain_concept\",   # prompt_name\n",
    "    {\"concept\": \"MCP Prompts capability\"},\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Issue 3 — `AttributeError: 'MultiServerMCPClient' has no attribute 'get_prompts'`**\n",
    "\n",
    "**Why:** The installed version of `langchain_mcp_adapters` does not yet expose `get_prompts()`.\n",
    "**Fix:** Outer `except AttributeError` handles this gracefully; prompt falls back to a static template.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue 4 — `call_tool()` returns wrong type**\n",
    "\n",
    "**Why:** `call_tool()` must return `list[TextContent]`, not a plain string (unlike `read_resource()`).\n",
    "**Fix:** `local_mcp_server_full.py` returns `[TextContent(type='text', text=result)]`.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue 5 — `ToolException: Input validation error: 'variables' is a required property`**\n",
    "\n",
    "**What happened:**\n",
    "The agent called `format_prompt` without a `variables` argument. The JSON schema validation rejected the call.\n",
    "\n",
    "**Why:**\n",
    "The original `variables` parameter had `\"type\": \"object\"` in the JSON schema. LLMs (including gpt-4o-mini) often struggle with nested object parameters — they tend to flatten the structure and pass `domain=\"LangChain\"` and `topic=\"agents\"` as top-level arguments instead of nesting them under a `variables` key.\n",
    "\n",
    "**Fix:**\n",
    "Changed `variables` from `\"type\": \"object\"` to `\"type\": \"string\"` in the tool's `inputSchema`. The description now instructs the LLM to pass a JSON string:\n",
    "```\n",
    "variables='{\"domain\": \"LangChain\", \"topic\": \"agents\"}'\n",
    "```\n",
    "The `call_tool` handler uses `json.loads()` to parse it, with a dict fallback for safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_intro",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 7 (Optional): Compare MCP vs Direct API Integration**\n",
    "\n",
    "**Objective:** Understand when to use MCP vs direct API calls.\n",
    "\n",
    "The agent, the queries, and the tool logic are **identical** in both approaches.  \n",
    "The only difference is **where the tools come from**:\n",
    "- **MCP approach** (Steps 5 & 6) — tools live in a separate server process, loaded at runtime via the MCP protocol\n",
    "- **Direct approach** (this step) — tools are plain Python functions decorated with `@tool`, defined inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "step7_direct_tools",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct tools defined: ['estimate_tokens', 'chunk_text', 'format_prompt']\n",
      "No MCP server, no subprocess, no client setup needed.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# ── Direct LangChain tools — same logic as local_mcp_server_full.py ───────\n",
    "# No server subprocess, no MCP client, no separate process.\n",
    "\n",
    "@tool\n",
    "def estimate_tokens(text: str) -> str:\n",
    "    \"\"\"Estimate the number of GPT tokens in a text string using the ~4 chars/token rule.\"\"\"\n",
    "    estimated = max(1, len(text) // 4)\n",
    "    word_count = len(text.split())\n",
    "    return (\n",
    "        f\"Estimated tokens: ~{estimated}\\n\"\n",
    "        f\"(Based on {len(text)} characters, {word_count} words; rule: 1 token ≈ 4 chars)\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 20) -> str:\n",
    "    \"\"\"Split a text into overlapping chunks by character count. Useful for RAG preparation.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        chunks.append(text[start:start + chunk_size])\n",
    "        start += chunk_size - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    lines = [f\"Total chunks: {len(chunks)}\\n\"]\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        lines.append(f\"Chunk {i}: {chunk!r}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "@tool\n",
    "def format_prompt(template: str, variables: str) -> str:\n",
    "    \"\"\"Fill a prompt template with {variable} placeholders. Pass variables as a JSON string.\"\"\"\n",
    "    try:\n",
    "        return template.format_map(json.loads(variables))\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "direct_tools = [estimate_tokens, chunk_text, format_prompt]\n",
    "print(f\"Direct tools defined: {[t.name for t in direct_tools]}\")\n",
    "print(\"No MCP server, no subprocess, no client setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "step7_direct_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/r4_2cb497nl1c31dzl76npj80000gp/T/ipykernel_5549/3567527120.py:4: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  direct_agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct Agent (no MCP) ===\n",
      "Tools: ['estimate_tokens', 'chunk_text', 'format_prompt']\n",
      "\n",
      "============================================================\n",
      "Query 1 [token estimation]\n",
      "============================================================\n",
      "The estimated number of tokens for the provided text is approximately 35.\n",
      "\n",
      "============================================================\n",
      "Query 2 [chunking]\n",
      "============================================================\n",
      "The text has been chunked into 3 pieces as follows:\n",
      "\n",
      "**Chunk 1:**  \n",
      "'LangChain is an open-source framework for building LLM-powered applications. It '\n",
      "\n",
      "**Chunk 2:**  \n",
      "'plications. It provides tools for chaining prompts, managing memory, and connect'\n",
      "\n",
      "**Chunk 3:**  \n",
      "'ry, and connecting to external data.'\n",
      "\n",
      "============================================================\n",
      "Query 3 [prompt formatting]\n",
      "============================================================\n",
      "The formatted prompt is: \n",
      "\n",
      "\"You are an expert in LangChain. Help the user understand agents.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "direct_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=direct_tools,\n",
    "    prompt=(\n",
    "        \"You are an expert LangChain assistant with LLM-dev utility tools. \"\n",
    "        \"Use estimate_tokens, chunk_text, and format_prompt when appropriate. \"\n",
    "        \"Be concise and include code examples.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"=== Direct Agent (no MCP) ===\")\n",
    "print(f\"Tools: {[t.name for t in direct_tools]}\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    (\n",
    "        \"token estimation\",\n",
    "        \"How many tokens is this text approximately: \"\n",
    "        \"'Retrieval-Augmented Generation (RAG) enhances LLM responses by retrieving \"\n",
    "        \"relevant context from a knowledge base before generating an answer.'? \"\n",
    "        \"Use the estimate_tokens tool.\",\n",
    "    ),\n",
    "    (\n",
    "        \"chunking\",\n",
    "        \"Chunk this text into pieces of 80 characters with 15 overlap: \"\n",
    "        \"'LangChain is an open-source framework for building LLM-powered applications. \"\n",
    "        \"It provides tools for chaining prompts, managing memory, and connecting to external data.'\",\n",
    "    ),\n",
    "    (\n",
    "        \"prompt formatting\",\n",
    "        \"Format this template: 'You are an expert in {domain}. Help the user understand {topic}.' \"\n",
    "        \"with domain='LangChain' and topic='agents'.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, (mode, question) in enumerate(test_queries, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query {i} [{mode}]\")\n",
    "    print(\"=\"*60)\n",
    "    result = await direct_agent.ainvoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_comparison",
   "metadata": {},
   "source": [
    "---\n",
    "##### Key Comparisons\n",
    "\n",
    "\n",
    "**MCP Integration**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Pros** | Standardized protocol — reusable across frameworks and languages |\n",
    "| | Built-in abstraction — agent doesn't care where tools run |\n",
    "| | Easy to add integrations — connect a new server without touching agent code |\n",
    "| | Tools can be remote — HTTP servers, cloud services, external APIs |\n",
    "| **Cons** | Additional abstraction layer — more moving parts |\n",
    "| | Requires server setup — separate file, subprocess, or HTTP service |\n",
    "| | Potential performance overhead — subprocess spawn or network round-trip per call |\n",
    "| **Best for** | Multiple integrations, standardized patterns, cross-framework compatibility, tools shared across apps |\n",
    "\n",
    "**Direct API Integration (`@tool`)**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Pros** | Direct control — no abstraction overhead |\n",
    "| | Simpler for single integrations — one file, no server |\n",
    "| | Better performance — tools run in-process |\n",
    "| | Easier to test and debug — plain Python functions |\n",
    "| **Cons** | Custom code per integration — must reimplement for each agent |\n",
    "| | Not reusable — tied to one codebase and language |\n",
    "| | More maintenance — tool logic lives inside the app |\n",
    "| | Inconsistent interfaces — each project does it differently |\n",
    "| **Best for** | Single specific integration, full API control needed, performance-critical applications, prototyping |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Aspect | MCP | Direct `@tool` |\n",
    "|---|---|---|\n",
    "| Tool discovery | Dynamic at runtime | Hardcoded |\n",
    "| Reusability | Any app, any language | Python / LangChain only |\n",
    "| Process overhead | Subprocess or HTTP | None — in-process |\n",
    "| Setup complexity | Server + client config | Just Python |\n",
    "| Update tools | Edit server, restart | Edit notebook / module |\n",
    "| Best scenario | Shared services, multi-app, cross-framework | Single-app, prototyping, speed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_docs_appendix",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Appendix — MCP Setup Reference\n",
    "\n",
    "The following documents the complete MCP (Model Context Protocol) configuration  \n",
    "completed for Claude Code on macOS, verified during this lab session.\n",
    "\n",
    "**Configuration File Locations**\n",
    "\n",
    "| Editor | Config File Path |\n",
    "|--------|-----------------|\n",
    "| Claude Code | `~/.claude/mcp-servers.json` |\n",
    "| Claude Desktop | `~/Library/Application Support/Claude/claude_desktop_config.json` |\n",
    "\n",
    "> Claude Code manages its own separate config file at `~/.claude/mcp-servers.json`.  \n",
    "> Both files were created during setup. The active config for Claude Code is  \n",
    "> `~/.claude/mcp-servers.json`.\n",
    "\n",
    "**Configured Server — Filesystem**\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Package | `@modelcontextprotocol/server-filesystem` |\n",
    "| Command | `npx` |\n",
    "| Scope | `/Users/dinabosmabuczynska/Desktop/bootcamp_env` |\n",
    "| Credentials | None required |\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"filesystem\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\n",
    "        \"-y\",\n",
    "        \"@modelcontextprotocol/server-filesystem\",\n",
    "        \"/Users/dinabosmabuczynska/Desktop/bootcamp_env\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Filesystem Server Tools**\n",
    "\n",
    "All tools are scoped to `bootcamp_env`. Claude cannot access files outside this path.\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| `read_file` | Read the contents of a file |\n",
    "| `write_file` | Write or create a file |\n",
    "| `list_directory` | List all files and folders in a directory |\n",
    "| `create_directory` | Create a new folder |\n",
    "| `move_file` | Move or rename a file |\n",
    "| `delete_file` | Delete a file |\n",
    "| `search_files` | Search for files by name pattern |\n",
    "| `get_file_info` | Get metadata about a file (size, dates, etc.) |\n",
    "\n",
    "**Tests Performed (Claude Code session)**\n",
    "\n",
    "1. **List directory** — Claude listed `bootcamp_env` and identified all folders and `mcp_extra.ipynb`.\n",
    "2. **Read file** — Claude read `mcp_test.txt` (content: `Hello MCP! This is a test file.`).\n",
    "3. **Write file** — Claude created `mcp_hello.txt` with content `MCP is working!`.\n",
    "4. **Query MCP tools** — Claude read `~/.claude/mcp-servers.json` and listed all 8 filesystem tools.\n",
    "\n",
    "\n",
    "**Issues Encountered and Solutions**\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| Config folder did not exist | Fresh Claude Code install | `mkdir -p ~/.claude` |\n",
    "| Library folder not visible in Finder | macOS hides Library by default | Terminal or Option key in Finder Go menu |\n",
    "| Claude Code used different config path | Separate config from Claude Desktop | Both files accepted; Claude Code uses `~/.claude/mcp-servers.json` |\n",
    "| `mcp_test.txt` not found on first attempt | File not yet created | Created via Terminal; Claude read it successfully on second attempt |\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- MCP servers run locally via `npx` — requires Node.js (`v24.13.1` used here).\n",
    "- The filesystem server is scoped to one directory — this is a security feature.\n",
    "- Always restart Claude Code after editing `mcp-servers.json`.\n",
    "- `mcp_hello.txt` and `mcp_test.txt` in `bootcamp_env/` remain as proof of successful read/write."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
